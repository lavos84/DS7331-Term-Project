{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# data transformation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "from pylab import *\n",
    "import matplotlib.patches as mpatches\n",
    "from PIL import Image\n",
    "\n",
    "# map plotting\n",
    "#import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# classification\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# statistics\n",
    "import scipy.stats as stats\n",
    "\n",
    "#for generating date_time \n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "1d3cae9b-ac78-4674-aad8-f39fa9cdf904"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The data set contains a variety of data types.  It is best to specify them before importing.\n",
    "# KEY: proper name\n",
    "# VALUE: pandas dtype\n",
    "datatypes = {\n",
    "    \"Borough\": \"category\",\n",
    "    \"Map Atlas\": \"category\",\n",
    "    \"Block\": \"category\",\n",
    "    \"Lot\": \"category\",\n",
    "    \"Address\": \"str\",\n",
    "    \"Parcel Name\": \"str\",\n",
    "    \"Agency\": \"category\",\n",
    "    \"Current Uses\": \"str\",\n",
    "    \"Total Area\": \"int\",\n",
    "    \"Open Petroleum Spill\": \"category\",\n",
    "    \"Govt Clean-Up Program\": \"category\",\n",
    "    \"Structure Completed\": \"float\",\n",
    "    \"Number Structures\": \"int\",\n",
    "    \"Total Gross Area Structures\": \"int\",\n",
    "    \"Ratio Building to Floor Area\": \"float\",\n",
    "    \"Allowable Building to Floor Area\": \"float\",\n",
    "    \"Land Use Category\": \"category\",\n",
    "    \"Community Board\": \"category\",\n",
    "    \"Census Tract\": \"category\",\n",
    "    \"Census Block\": \"category\",\n",
    "    \"School Dist\": \"category\",\n",
    "    \"Council District\": \"category\",\n",
    "    \"Postcode\": \"category\",\n",
    "    \"Fire Comp\": \"category\",\n",
    "    \"Health Area\": \"category\",\n",
    "    \"Health Ctr\": \"category\",\n",
    "    \"Police Prct\": \"category\",\n",
    "    \"Major Use\": \"category\",\n",
    "    \"Number of Easements\": \"int\",\n",
    "    \"Commercial Floor Area\": \"int\",\n",
    "    \"Residential Floor Area\": \"int\",\n",
    "    \"Office Floor Area\": \"int\",\n",
    "    \"Retail Floor Area\": \"int\",\n",
    "    \"Garage Floor Area\": \"int\",\n",
    "    \"Storage Floor Area\": \"int\",\n",
    "    \"Factory Floor Area\": \"int\",\n",
    "    \"Other Floor Area\": \"int\",\n",
    "    \"Num Floors\": \"float\",\n",
    "    \"Residential Units\": \"int\",\n",
    "    \"Residential and Non-Residential Units\": \"int\",\n",
    "    \"Lot Front\": \"float\",\n",
    "    \"Lot Depth\": \"float\",\n",
    "    \"Bldg Front\": \"float\",\n",
    "    \"Bldg Depth\": \"float\",\n",
    "    \"Proximity Code\": \"category\",\n",
    "    \"Irr Lot Code\": \"category\",\n",
    "    \"Lot Type Code\": \"category\",\n",
    "    \"Bsmt Code\": \"category\",\n",
    "    \"Assess Land\": \"float\",\n",
    "    \"Exempt Land\": \"float\",\n",
    "    \"Exempt Tot\": \"float\",\n",
    "    \"Year Alter 1\": \"float\",\n",
    "    \"Year Alter 2\": \"float\",\n",
    "    \"His Dist\": \"category\",\n",
    "    \"Landmark\": \"str\",\n",
    "    \"Condominium Number\": \"category\",\n",
    "    \"Coordinates\": \"str\",\n",
    "    \"E-Designation Number\": \"category\",\n",
    "    \"Industrial Business Zone\": \"category\",\n",
    "    \"Zone Dist\": \"category\",\n",
    "    \"Zone Dist 2\": \"category\",\n",
    "    \"Overlay 1\": \"category\",\n",
    "    \"Overlay 2\": \"category\",\n",
    "    \"SP Dist 1\": \"category\",\n",
    "    \"SP Dist 2\": \"category\",\n",
    "    \"Potential Urban Ag\": \"category\",\n",
    "    \"Contact\": \"str\",\n",
    "    \"EDC % Occupied\": \"float\",\n",
    "    \"Pluto Version\": \"category\",\n",
    "    \"Latitude\": \"float\",\n",
    "    \"Longitude\": \"float\",\n",
    "    \"BIN\": \"str\",\n",
    "    \"NTA\": \"category\"\n",
    "}\n",
    "\n",
    "# Similarly, each column will have to have a different method of handling missing values.\n",
    "# KEY: proper name\n",
    "# VALUE: missing value type\n",
    "nan = np.nan\n",
    "nas = {\n",
    "    \"Borough\": \"\",\n",
    "    \"Map Atlas\": \"\",\n",
    "    \"Block\": nan,\n",
    "    \"Lot\": nan,\n",
    "    \"Address\": \"\",\n",
    "    \"Parcel Name\": \"\",\n",
    "    \"Agency\": \"\",\n",
    "    \"Current Uses\": \"\",\n",
    "    \"Total Area\": nan,\n",
    "    \"Open Petroleum Spill\": \"\",\n",
    "    \"Govt Clean-Up Program\": \"\",\n",
    "    \"Structure Completed\": nan,\n",
    "    \"Number Structures\": nan,\n",
    "    \"Total Gross Area Structures\": nan,\n",
    "    \"Ratio Building to Floor Area\": nan,\n",
    "    \"Allowable Building to Floor Area\": nan,\n",
    "    \"Land Use Category\": \"\",\n",
    "    \"Community Board\": \"\",\n",
    "    \"Census Tract\": nan,\n",
    "    \"Census Block\": \"\",\n",
    "    \"School Dist\": \"\",\n",
    "    \"Council District\": \"\",\n",
    "    \"Postcode\": \"\",\n",
    "    \"Fire Comp\": \"\",\n",
    "    \"Health Area\": \"\",\n",
    "    \"Health Ctr\": \"\",\n",
    "    \"Police Prct\": \"\",\n",
    "    \"Major Use\": \"\",\n",
    "    \"Number of Easements\": nan,\n",
    "    \"Commercial Floor Area\": nan,\n",
    "    \"Residential Floor Area\": nan,\n",
    "    \"Office Floor Area\": nan,\n",
    "    \"Retail Floor Area\": nan,\n",
    "    \"Garage Floor Area\": nan,\n",
    "    \"Storage Floor Area\": nan,\n",
    "    \"Factory Floor Area\": nan,\n",
    "    \"Other Floor Area\": nan,\n",
    "    \"Num Floors\": nan,\n",
    "    \"Residential Units\": nan,\n",
    "    \"Residential and Non-Residential Units\": nan,\n",
    "    \"Lot Front\": nan,\n",
    "    \"Lot Depth\": nan,\n",
    "    \"Bldg Front\": nan,\n",
    "    \"Bldg Depth\": nan,\n",
    "    \"Proximity Code\": \"\",\n",
    "    \"Irr Lot Code\": \"\",\n",
    "    \"Lot Type Code\": \"\",\n",
    "    \"Bsmt Code\": \"\",\n",
    "    \"Assess Land\": nan,\n",
    "    \"Exempt Land\": nan,\n",
    "    \"Exempt Tot\": nan,\n",
    "    \"Year Alter 1\": nan,\n",
    "    \"Year Alter 2\": nan,\n",
    "    \"His Dist\": \"\",\n",
    "    \"Landmark\": \"\",\n",
    "    \"Condominium Number\": nan,\n",
    "    \"Coordinates\": \"\",\n",
    "    \"E-Designation Number\": \"\",\n",
    "    \"Industrial Business Zone\": \"\",\n",
    "    \"Zone Dist\": \"\",\n",
    "    \"Zone Dist 2\": \"\",\n",
    "    \"Overlay 1\": \"\",\n",
    "    \"Overlay 2\": \"\",\n",
    "    \"SP Dist 1\": \"\",\n",
    "    \"SP Dist 2\": \"\",\n",
    "    \"Potential Urban Ag\": \"\",\n",
    "    \"Contact\": \"\",\n",
    "    \"EDC % Occupied\": nan,\n",
    "    \"Pluto Version\": \"\",\n",
    "    \"Latitude\": nan,\n",
    "    \"Longitude\": nan,\n",
    "    \"BIN\": \"\",\n",
    "    \"NTA\": \"\"\n",
    "}\n",
    "# Read in the data.\n",
    "nyc = pd.read_csv(\n",
    "    \"./Data/city-owned-and-leased-property-local-law-48-of-2011.csv\",\n",
    "    dtype=datatypes,\n",
    "    na_values=nas)\n",
    "\n",
    "# Change the type of Structure Complete, Year Alter 1 and Year Alter 2\n",
    "nyc[\"Structure Completed\"] = nyc[\"Structure Completed\"].fillna(0)\n",
    "nyc[\"Year Alter 1\"] = nyc[\"Year Alter 1\"].fillna(0)\n",
    "nyc[\"Year Alter 2\"] = nyc[\"Year Alter 2\"].fillna(0)\n",
    "nyc[\"Structure Completed\"] = nyc[\"Structure Completed\"].apply(pd.to_numeric, downcast='integer')\n",
    "nyc[\"Year Alter 1\"] = nyc[\"Year Alter 1\"].apply(pd.to_numeric, downcast='integer')\n",
    "nyc[\"Year Alter 2\"] = nyc[\"Year Alter 2\"].apply(pd.to_numeric, downcast='integer')\n",
    "\n",
    "# Preview the data\n",
    "#nyc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 15\n",
    "HEIGHT = 15\n",
    "\n",
    "# create proper names to label graphs\n",
    "proper_names = list(nyc)\n",
    "\n",
    "#replace space with underscore\n",
    "nyc.columns = nyc.columns.str.replace(' ', '_')\n",
    "\n",
    "#make lowercase\n",
    "nyc.columns = nyc.columns.str.lower()\n",
    "\n",
    "#create proper name dictionary for plotting\n",
    "proper_name_dict = dict(zip(list(nyc.columns), proper_names)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Classification\n",
    "\n",
    "_You are to build upon the predictive analysis (classification) that you already completed in the\n",
    "previous mini-project, adding additional modeling from new classification algorithms as well as\n",
    "more explanations that are inline with the CRISP-DM framework. You should use appropriate cross\n",
    "validation for all of your analysis (explain your chosen method of performance validation in detail).\n",
    "Try to use as much testing data as possible in a realistic manner (you should define what you think\n",
    "is realistic and why)._\n",
    "\n",
    "_This report is worth 20% of the final grade. Please upload a report (one per team) with all code\n",
    "used, visualizations, and text in a single document. The format of the document can be PDF,\n",
    "*.ipynb, or HTML. You can write the report in whatever format you like, but it is easiest to turn in the\n",
    "rendered Jupyter notebook. The results should be reproducible using your report. Please carefully\n",
    "describe every assumption and every step in your report._\n",
    "\n",
    "**Dataset Selection**\n",
    "_Select a dataset identically to the way you selected for the first project work week and mini-project.\n",
    "You are not required to use the same dataset that you used in the past, but you are encouraged.\n",
    "You must identify two tasks from the dataset to regress or classify. That is:_\n",
    "\n",
    "* _two classification tasks OR_\n",
    "* _two regression tasks OR_\n",
    "* _one classification task and one regression task_\n",
    "\n",
    "_For example, if your dataset was from the diabetes data you might try to predict two tasks: (1)\n",
    "classifying if a patient will be readmitted within a 30 day period or not, and (2) regressing what the\n",
    "total number of days a patient will spend in the hospital, given their history and specifics of the\n",
    "encounter like tests administered and previous admittance._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "* **[10 points]** _Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis._\n",
    "* **[5 points]** _Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating the Data by Date\n",
    "\n",
    "In section 1.2.1.2 we restricted the data to only those entries more recent than January 1st, 2017.  This time we would like to analyze if there is a statistically significant difference between the Assessed Land Value and Zone District 1 from year to year.  If there is, we will need to proceed taking into consideration the serial correlation associated with time series data.  If not, we will examine the year which contains the fewest missing values within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the \"Date Created\" column to datetime format\n",
    "nyc.date_created = pd.to_datetime(nyc['date_created'])\n",
    "\n",
    "# Subset the data by date\n",
    "nyc_2011 = nyc.loc[nyc.date_created < pd.to_datetime(\"1/11/2012\")].copy()\n",
    "nyc_2013 = nyc.loc[(nyc.date_created >= pd.to_datetime(\"1/1/2013\")) & \n",
    "                   (nyc.date_created < pd.to_datetime(\"1/11/2014\"))].copy()\n",
    "nyc_2015 = nyc.loc[(nyc.date_created >= pd.to_datetime(\"1/1/2015\")) & \n",
    "                   (nyc.date_created < pd.to_datetime(\"1/11/2016\"))].copy()\n",
    "nyc_2017 = nyc.loc[nyc.date_created >= pd.to_datetime(\"1/1/2017\")].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessed Land ANOVA\n",
    "\n",
    "Now that we've broken the data into four distinct groups according to their Date Created feature, let's see if there is any significant difference between the Assessed Land Value over the four years of collected data.  We will conduct an ANOVA test with:\n",
    "\n",
    "_$H_{0}$: There is no difference in the Assessed Land Values for the four years of collected data._<br>\n",
    "_$H_{a}$: The Assessed Land Value is different for at least one year of collected data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-value:  0.1085\n",
      "p-value:  0.9552\n"
     ]
    }
   ],
   "source": [
    "# Conduct an ANOVA on the Assessed Land Value dropping missing observations\n",
    "f_value, p_value = stats.f_oneway(nyc_2011.assess_land.dropna(), \n",
    "                                  nyc_2013.assess_land.dropna(), \n",
    "                                  nyc_2015.assess_land.dropna(), \n",
    "                                  nyc_2017.assess_land.dropna())\n",
    "print(\"F-value: \", round(f_value, 4))\n",
    "print(\"p-value: \", round(p_value, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the small F-value (.1085) and high p-value (.9552) that we fail to reject _$H_{0}$_ and conclude that there is no difference in the Assessed Land Value from year to year.  Because of this fact, we will not consider time series data and instead use the year with the fewest missing values contained within its data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of missing values from 2011:  256176\n",
      "Total number of missing values from 2013:  230934\n",
      "Total number of missing values from 2015:  251047\n",
      "Total number of missing values from 2017:  247675\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of missing values from 2011: \", sum(sum(nyc_2011.isna())))\n",
    "print(\"Total number of missing values from 2013: \", sum(sum(nyc_2013.isna())))\n",
    "print(\"Total number of missing values from 2015: \", sum(sum(nyc_2015.isna())))\n",
    "print(\"Total number of missing values from 2017: \", sum(sum(nyc_2017.isna())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would appear that the data from 2013 contains 16,741 fewer missing values than its next closest competitor, 2017.  For the regression task, we will proceed with the 2013 data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zone District 1 $\\chi^{2}$ Test\n",
    "\n",
    "While there is no difference in the Assessed Land Value from year to year, we should also test to see if this is the case with Zone District 1.  The best way to do this is via $\\chi^{2}$ test.  The four year categories will represent the columns for the test, and the unique district zoning codes will constitute the rows.  In order to ensure an apples to apples comparison, we must make sure that each year of data has the same number of unique codes, and replace missing values with 0's since there would be none of that code if it were missing from the entire year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             p_value\n",
      "PARKUS  6.662681e-27\n",
      "R1-2    1.248756e-12\n",
      "R5B     3.734167e-10\n",
      "R6A     6.083213e-08\n",
      "R3X     1.897966e-05\n",
      "R6      2.709212e-05\n",
      "R3A     4.982276e-05\n",
      "M2-3    5.848091e-05\n",
      "PARK    3.692810e-04\n",
      "C6-7    6.658455e-04\n",
      "R5      1.694733e-03\n",
      "R3-1    2.544257e-03\n",
      "C4-4L   4.384525e-03\n",
      "R6B     6.769558e-03\n",
      "R7-2    7.063688e-03\n",
      "C2-8    7.871353e-03\n",
      "BPC     1.309845e-02\n",
      "R7A     1.545017e-02\n",
      "C2-7    2.498833e-02\n",
      "R5D     2.901622e-02\n",
      "C4-5    3.231522e-02\n"
     ]
    }
   ],
   "source": [
    "# The original nyc data set contains all of the unique values for zoning district\n",
    "# Using this fact we can make sure each year contains the same series index\n",
    "zd_obs = pd.DataFrame(data = {\"year_2011\":nyc_2011.zone_dist_1.value_counts(), \n",
    "                              \"year_2013\":nyc_2013.zone_dist_1.value_counts(),\n",
    "                              \"year_2015\":nyc_2015.zone_dist_1.value_counts(),\n",
    "                              \"year_2017\":nyc_2017.zone_dist_1.value_counts()},\n",
    "                      index = nyc.zone_dist_1.value_counts().index)\n",
    "\n",
    "# Now make sure any missing values are replaced with 0's.\n",
    "zd_obs = zd_obs.fillna(0)\n",
    "\n",
    "# Determine the expected frequencies\n",
    "zd_exp = pd.DataFrame(data = stats.contingency.expected_freq(zd_obs),\n",
    "                      columns = [\"year_2011\", \"year_2013\", \"year_2015\", \"year_2017\"],\n",
    "                      index = nyc.zone_dist_1.value_counts().index)\n",
    "\n",
    "# Conduct the Chi-square test\n",
    "chi_value, p_value = stats.chisquare(zd_obs, zd_exp, axis = 1)\n",
    "zd_p = pd.DataFrame(data = p_value,\n",
    "                   columns = [\"p_value\"],\n",
    "                   index = nyc.zone_dist_1.value_counts().index)\n",
    "\n",
    "# Output the index labels where we reject the null hypothesis\n",
    "print(zd_p[zd_p.p_value <= .05].sort_values(by = \"p_value\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceeding output represents the p-values for those zoning codes in which we reject $H_{0}$ at the $\\alpha$ = 0.05 significance level.  Out of the 146 unique zoning codes, only the above 21 had statistically significantly deviation from their expected values; and we fail to reject the null hypothesis for the other 125 codes and conclude that they do not vary significantly from year to year.  What is interesting is the remarkably low p-value for the zoning code \"PARKUS.\"  This should certainly be investigated further to gain some understanding of why this particular zoning code is so whimsical over the years.\n",
    "\n",
    "With 85.62% of zoning codes failing to reject $H_{0}$, it is probably safe to proceed with analyzing just the 2013 data for Zone Dist 1 as well.  We just have to recognize that any classification model would have to properly weight other features in order to capture the seemingly dynamic nature of the above 21 zoning codes for year data other than 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Class Variables\n",
    "\n",
    "Recognizing that 2013 had the fewest number of missing values, we will now take a deeper look at the number of missing values from individual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Percent Null</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Current Uses</td>\n",
       "      <td>0.080239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Zone Dist 1</td>\n",
       "      <td>0.218834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Lot Type Code</td>\n",
       "      <td>0.284485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Bsmt Code</td>\n",
       "      <td>0.284485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Irr Lot Code</td>\n",
       "      <td>0.284485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Proximity Code</td>\n",
       "      <td>0.284485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Major Use</td>\n",
       "      <td>0.284485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Address</td>\n",
       "      <td>0.299074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Map Atlas</td>\n",
       "      <td>0.342840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Land Use Category</td>\n",
       "      <td>1.597491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>School Dist</td>\n",
       "      <td>12.692392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Fire Comp</td>\n",
       "      <td>14.537895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Census Tract</td>\n",
       "      <td>15.792545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Parcel Name</td>\n",
       "      <td>27.463710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Census Block</td>\n",
       "      <td>47.618353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Longitude</td>\n",
       "      <td>57.925450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Latitude</td>\n",
       "      <td>57.925450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>NTA</td>\n",
       "      <td>57.925450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>BIN</td>\n",
       "      <td>61.135021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Potential Urban Ag</td>\n",
       "      <td>72.040266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>SP Dist 1</td>\n",
       "      <td>87.249252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Overlay 1</td>\n",
       "      <td>89.029105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Zone Dist 2</td>\n",
       "      <td>94.711503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Industrial Business Zone</td>\n",
       "      <td>94.850098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Open Petroleum Spill</td>\n",
       "      <td>95.462835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>E-Designation Number</td>\n",
       "      <td>97.432344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Landmark</td>\n",
       "      <td>98.351448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>His Dist</td>\n",
       "      <td>98.635933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Overlay 2</td>\n",
       "      <td>99.897877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Govt Clean-Up Program</td>\n",
       "      <td>99.934350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>SP Dist 2</td>\n",
       "      <td>99.970822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Health Ctr</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Health Area</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Feature  Percent Null\n",
       "8               Current Uses      0.080239\n",
       "60               Zone Dist 1      0.218834\n",
       "47             Lot Type Code      0.284485\n",
       "48                 Bsmt Code      0.284485\n",
       "46              Irr Lot Code      0.284485\n",
       "45            Proximity Code      0.284485\n",
       "28                 Major Use      0.284485\n",
       "5                    Address      0.299074\n",
       "2                  Map Atlas      0.342840\n",
       "17         Land Use Category      1.597491\n",
       "21               School Dist     12.692392\n",
       "24                 Fire Comp     14.537895\n",
       "19              Census Tract     15.792545\n",
       "6                Parcel Name     27.463710\n",
       "20              Census Block     47.618353\n",
       "71                 Longitude     57.925450\n",
       "70                  Latitude     57.925450\n",
       "73                       NTA     57.925450\n",
       "72                       BIN     61.135021\n",
       "66        Potential Urban Ag     72.040266\n",
       "64                 SP Dist 1     87.249252\n",
       "62                 Overlay 1     89.029105\n",
       "61               Zone Dist 2     94.711503\n",
       "59  Industrial Business Zone     94.850098\n",
       "10      Open Petroleum Spill     95.462835\n",
       "58      E-Designation Number     97.432344\n",
       "55                  Landmark     98.351448\n",
       "54                  His Dist     98.635933\n",
       "63                 Overlay 2     99.897877\n",
       "11     Govt Clean-Up Program     99.934350\n",
       "65                 SP Dist 2     99.970822\n",
       "26                Health Ctr    100.000000\n",
       "25               Health Area    100.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the percentage of missing values from the 2013 data\n",
    "na_percent = (nyc_2013.isnull().sum() / nyc_2013.shape[0] * 100)\n",
    "\n",
    "# change to dataframe for easier processing\n",
    "na_percent = pd.DataFrame({\n",
    "    'Feature': na_percent.index,\n",
    "    'Percent Null': na_percent.values\n",
    "})\n",
    "\n",
    "# replace with proper name\n",
    "na_percent = na_percent.replace(proper_name_dict)\n",
    "\n",
    "# select only vlaues that are greater than 0. this makes the visual useful\n",
    "na_percent = na_percent[na_percent['Percent Null'] > 0 ].sort_values(by='Percent Null',ascending=True)\n",
    "na_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above displays all features that have at least one missing value (Percent Null > 0).  Features with greater than 70% missing values are too scarce to provide any meaningful insight, unless these attributes can be safely interpolated.  Additinally, the features Parcel Name, Map Atlas, Address, Agency, Postcode, Coordinates, Condominium Number, E-Designation Number, Pluto Version, Contact, BIN, NTA, Longitude and Lattitude which mostly are string datatypes consist primarily of unuseful information can be simply removed.\n",
    "\n",
    "The Census Tract and Census Block features divide the boroughs and Census Tracts, respectively, into smaller sub-divisions for the purpose of 10 year census data.  These sub divisions would add hundreds (possibly thousands) of additional columns if they were one-hot encoded and convolute the classification task.  They too will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing dimensionality (parcel name, address, agency, contact, postcode, coordinates, latitude,\n",
    "# longitude, nta, e-designation number and condominium number) and also variables with over 80%\n",
    "# missing values.\n",
    "for col in ['parcel_name',\n",
    "            'map_atlas',\n",
    "            'address',\n",
    "            'agency',\n",
    "            'census_tract',\n",
    "            'census_block',\n",
    "            'sp_dist_1',\n",
    "            'sp_dist_2',\n",
    "            'postcode',\n",
    "            'overlay_1',\n",
    "            'overlay_2',\n",
    "            'his_dist',\n",
    "            'landmark',\n",
    "            'health_area',\n",
    "            'health_ctr',\n",
    "            'zone_dist_2',\n",
    "            'potential_urban_ag',\n",
    "            'industrial_business_zone',\n",
    "            'edc_%_occupied',\n",
    "            'govt_clean-up_program',\n",
    "            'open_petroleum_spill',\n",
    "            'latitude',\n",
    "            'longitude',\n",
    "            'condominium_number',\n",
    "            'coordinates',\n",
    "            'e-designation_number',\n",
    "            'pluto_version',\n",
    "            'contact',\n",
    "            'bin',\n",
    "            'nta']:\n",
    "    if col in nyc_2013: del nyc_2013[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop observations with null values from the remaining features < 70% null\n",
    "nyc_2013.dropna(subset = ['current_uses',\n",
    "                          'zone_dist_1',\n",
    "                          'irr_lot_code',\n",
    "                          'bsmt_code', \n",
    "                          'lot_type_code',\n",
    "                          'proximity_code',\n",
    "                          'major_use',\n",
    "                          'land_use_category',\n",
    "                          'school_dist',\n",
    "                          'fire_comp',],\n",
    "                inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2013 data has thus been reduced from 13,709 to 11,496 observations and 74 features down to 45 and contains no missing values.  Of the remaining features, the Structure Completed attribute identifies the year that the structure was completed on the lot.  We now engineer a new feature that provides a simple \"Yes\" if the structure was completed, or \"No\" if the year is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No     7665\n",
      "Yes    3831\n",
      "Name: structure_completed_2, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create the new column\n",
    "nyc_2013[\"structure_completed_2\"] = nyc_2013.structure_completed\n",
    "\n",
    "# Convert the type to string\n",
    "nyc_2013.structure_completed_2 = nyc_2013.structure_completed_2.astype(str)\n",
    "\n",
    "# Assign the data\n",
    "nyc_2013.loc[nyc_2013.structure_completed == 0, \"structure_completed_2\"] = \"No\"\n",
    "nyc_2013.loc[nyc_2013.structure_completed != 0, \"structure_completed_2\"] = \"Yes\"\n",
    "\n",
    "# Convert back to categorical\n",
    "nyc_2013.structure_completed_2 = nyc_2013.structure_completed_2.astype(\"category\")\n",
    "\n",
    "# Output the results\n",
    "print(nyc_2013.structure_completed_2.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding this feature provides an \"at a glance\" view if the lot has a completed structure on it.  Additionally, data older than 1985 rapidly becomes less reliable when determining the year completed, so this new feature will not concern itself with inaccuracies in the actual year of completion.\n",
    "\n",
    "There are 137 categories for zone dist 1.  Let's create a new feature that reduces the number of categories to something easier.  The NYC Zoning Map is split into three main zoning districts: Commercial (C), Residential (R), and Manufacturing (M).  Within each of these districts, low, medium, and high-density districts are mapped out. (https://www.reonomy.com/blog/post/guide-to-nyc-zoning).  There is also another category so called PARK.  Hence, we divide our zoning districs into 4 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residential      6858\n",
      "Park             2551\n",
      "Manufacturing    1281\n",
      "Commercial        806\n",
      "Name: new_zone_dist, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Just like as above...\n",
    "nyc_2013[\"new_zone_dist\"] = nyc_2013.zone_dist_1\n",
    "nyc_2013.new_zone_dist = nyc_2013.new_zone_dist.astype(str)\n",
    "# Only FANCIER!\n",
    "for first_letter, zone in zip([\"C\", \"R\", \"M\", \"P\"], [\"Commercial\", \"Residential\", \"Manufacturing\", \"Park\"]):\n",
    "    nyc_2013.loc[nyc_2013.zone_dist_1.astype(str).apply(lambda x: x[0]) == first_letter, \"new_zone_dist\"] = zone\n",
    "nyc_2013.new_zone_dist = nyc_2013.new_zone_dist.astype(\"category\")\n",
    "\n",
    "#Output the results\n",
    "print(nyc_2013.new_zone_dist.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The New Zone Dist feature substantially reduces the number of categories in Zone District 1 from 137 down to 4.  Although we may potentially lose some useful information by omitting everything but the first letter of the zoning code, the intricacies within those extra characters would likely convolute any classification algorithm we attempt.\n",
    "\n",
    "The Major Use feature contains a wealth of information, but has too many levels within it to be useful.  Appendix C of https://www1.nyc.gov/assets/planning/download/pdf/data-maps/open-data/pluto_datadictionary.pdf (pg. 49) outlines each individual code under the category it falls into.  Instead of using the entire code, we'll use just the first letter in the alphanumeric code.  While some information will be lost in this process, the first character is where the bulk of the information about this code lies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A      34\n",
      "B      19\n",
      "C     255\n",
      "D      39\n",
      "E      63\n",
      "F      41\n",
      "G     521\n",
      "H       5\n",
      "I      98\n",
      "J       9\n",
      "K      78\n",
      "L       3\n",
      "M      31\n",
      "N      52\n",
      "O     238\n",
      "P     287\n",
      "Q    2803\n",
      "R       2\n",
      "S      33\n",
      "T     146\n",
      "U     448\n",
      "V    4569\n",
      "W    1210\n",
      "Y     436\n",
      "Z      76\n",
      "Name: bldg_class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Just like as above...\n",
    "nyc_2013[\"bldg_class\"] = nyc_2013.major_use\n",
    "nyc_2013.bldg_class = nyc_2013.bldg_class.astype(str)\n",
    "# Now iterate through every letter in the alphabet, replacing the code with just the letter\n",
    "for first_letter in [chr(i) for i in range(ord('A'),ord('Z')+1)]:\n",
    "    nyc_2013.loc[nyc_2013.major_use.astype(str).apply(lambda x: x[0]) == first_letter, \"bldg_class\"] = first_letter\n",
    "nyc_2013.bldg_class = nyc_2013.bldg_class.astype(\"category\")\n",
    "\n",
    "# We can safely drop the Major Use category now.\n",
    "\n",
    "#Output the results\n",
    "print(nyc_2013.bldg_class.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no category for \"X\" in the PLUTO Data Dictionary which is why we don't see any counts for it.  This step substantially reduces the number of columns that will be generated when we create dummy variables for each level in this categorical variable.\n",
    "\n",
    "The last feature would like to revisit is the Assessed Land Value. We have already identified this feature as our target for regression, but by creating different bins constructed from the summary statistic quartiles, we can also look at this from a classification perspective. We create a new feature to do jus that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_zone_dist  assess_range\n",
      "Commercial     low               73\n",
      "               medium           200\n",
      "               high             533\n",
      "Manufacturing  low              235\n",
      "               medium           521\n",
      "               high             525\n",
      "Park           low              497\n",
      "               medium           976\n",
      "               high            1078\n",
      "Residential    low             2757\n",
      "               medium          2211\n",
      "               high            1890\n",
      "Name: assess_range, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create a new column that bins assess_land into 3 categories\n",
    "nyc_2013[\"assess_range\"] = pd.qcut(nyc.assess_land, 3, labels=['low', 'medium', 'high'])\n",
    "\n",
    "# Display the results by new_zone_dist\n",
    "nyc_grouped = nyc_2013.groupby(['new_zone_dist', 'assess_range'])\n",
    "print(nyc_grouped.assess_range.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output tells us that the majority of properties belong the the residential zoning code, and that every zoning code contains more \"High\" assessed land values than any other category within their group.  The last step in the Data Preparation phase is to one-hot encode our categorical variables.  Upon closer inspection, there are a few more columns that may be excluded from the final data set.  They are:\n",
    "\n",
    "* Date Created\n",
    "* Block\n",
    "* Lot\n",
    "* Current Uses\n",
    "* Major Use (before the dimensionality reduction)\n",
    "* Zone District 1 (before the dimensionality reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after one-hot encoding categorical variables:  (11496, 663)\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical variables.  \n",
    "# We leave out Land Use Category and Structure Completed 2 because we'll use it in a visual later\n",
    "categorical_fields = list(nyc_2013.drop(\n",
    "    columns = [\"block\", \n",
    "               \"lot\", \n",
    "               \"major_use\",\n",
    "               \"new_zone_dist\",\n",
    "               \"zone_dist_1\", \n",
    "               \"assess_range\", \n",
    "               \"land_use_category\",\n",
    "               \"structure_completed_2\"]).select_dtypes(\n",
    "    include=['category']).columns)\n",
    "\n",
    "# Creating dummy variables for their levels\n",
    "nyc_2013 = pd.get_dummies(nyc_2013, columns = categorical_fields).drop(\n",
    "    columns = [\"date_created\", \n",
    "               \"block\", \"lot\", \n",
    "               \"current_uses\", \n",
    "               \"major_use\", \n",
    "               \"zone_dist_1\"])\n",
    "    \n",
    "print(\"Shape after one-hot encoding categorical variables: \", nyc_2013.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data for ease of loading.\n",
    "#nyc_2013.to_csv(\"./Data/nyc_2013.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe the Data\n",
    "\n",
    "After the addition of the Assessed Land Value Range feature, we have a complete picture of our data set.  Summary statistics for all features are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_area</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154970</td>\n",
       "      <td>2.40516e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>2008.75</td>\n",
       "      <td>6000</td>\n",
       "      <td>35841.5</td>\n",
       "      <td>2.14756e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>structure_completed</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>648.238</td>\n",
       "      <td>917.127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1926</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_structures</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.60047</td>\n",
       "      <td>2.52583</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_gross_area_structures</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34745.2</td>\n",
       "      <td>341706</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6063.75</td>\n",
       "      <td>2.76e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ratio_building_to_floor_area</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.634766</td>\n",
       "      <td>2.00386</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "      <td>59.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allowable_building_to_floor_area</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.55182</td>\n",
       "      <td>1.98763</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.43</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>land_use_category</th>\n",
       "      <td>11496</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>4569</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_easements</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0473208</td>\n",
       "      <td>0.295869</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>commercial_floor_area</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33039.6</td>\n",
       "      <td>337206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3200</td>\n",
       "      <td>2.76e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>residential_floor_area</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1704.76</td>\n",
       "      <td>47140.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.97974e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>office_floor_area</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9664.4</td>\n",
       "      <td>67941.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.68548e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retail_floor_area</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>834.764</td>\n",
       "      <td>12928.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>537154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>garage_floor_area</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1766.51</td>\n",
       "      <td>30984.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.67743e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storage_floor_area</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>596.883</td>\n",
       "      <td>16081.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.19715e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>factory_floor_area</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>583.885</td>\n",
       "      <td>15575.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.32459e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_floor_area</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19450.6</td>\n",
       "      <td>316782</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.76e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_floors</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.13944</td>\n",
       "      <td>3.0113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>residential_units</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.79828</td>\n",
       "      <td>116.382</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>residential_and_non-residential_units</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.08968</td>\n",
       "      <td>124.34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lot_front</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>163.082</td>\n",
       "      <td>371.339</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>60</td>\n",
       "      <td>200</td>\n",
       "      <td>9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lot_depth</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200.47</td>\n",
       "      <td>317.502</td>\n",
       "      <td>0</td>\n",
       "      <td>93.9925</td>\n",
       "      <td>100.25</td>\n",
       "      <td>200.02</td>\n",
       "      <td>9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_front</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.1312</td>\n",
       "      <td>89.5937</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1394.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_depth</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.6058</td>\n",
       "      <td>111.604</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>7360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assess_land</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.81569e+06</td>\n",
       "      <td>3.70013e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>2094</td>\n",
       "      <td>51741</td>\n",
       "      <td>424012</td>\n",
       "      <td>2.85512e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exempt_land</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.67277e+06</td>\n",
       "      <td>3.69734e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>257971</td>\n",
       "      <td>2.85512e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exempt_tot</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.48342e+06</td>\n",
       "      <td>6.5389e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>597375</td>\n",
       "      <td>5.535e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year_alter_1</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203.261</td>\n",
       "      <td>603.037</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year_alter_2</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.5978</td>\n",
       "      <td>295.527</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>structure_completed_2</th>\n",
       "      <td>11496</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>7665</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_zone_dist</th>\n",
       "      <td>11496</td>\n",
       "      <td>4</td>\n",
       "      <td>Residential</td>\n",
       "      <td>6858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bsmt_code_0</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0533229</td>\n",
       "      <td>0.224686</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bsmt_code_1</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00730689</td>\n",
       "      <td>0.0851712</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bsmt_code_2</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0728079</td>\n",
       "      <td>0.259832</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bsmt_code_4</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000173974</td>\n",
       "      <td>0.0131893</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bsmt_code_5</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.866388</td>\n",
       "      <td>0.340249</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_A</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00295755</td>\n",
       "      <td>0.0543052</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_B</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00165275</td>\n",
       "      <td>0.0406222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_C</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0221816</td>\n",
       "      <td>0.14728</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_D</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00339248</td>\n",
       "      <td>0.0581487</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_E</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00548017</td>\n",
       "      <td>0.0738282</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_F</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00356646</td>\n",
       "      <td>0.0596158</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_G</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0453201</td>\n",
       "      <td>0.208014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_H</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000434934</td>\n",
       "      <td>0.0208514</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_I</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0085247</td>\n",
       "      <td>0.0919389</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_J</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000782881</td>\n",
       "      <td>0.0279703</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_K</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00678497</td>\n",
       "      <td>0.0820946</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_L</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00026096</td>\n",
       "      <td>0.0161529</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_M</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00269659</td>\n",
       "      <td>0.0518609</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_N</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00452331</td>\n",
       "      <td>0.0671062</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_O</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0207029</td>\n",
       "      <td>0.142394</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_P</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0249652</td>\n",
       "      <td>0.156026</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_Q</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.243824</td>\n",
       "      <td>0.429406</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_R</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000173974</td>\n",
       "      <td>0.0131893</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_S</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00287056</td>\n",
       "      <td>0.053503</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_T</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0127001</td>\n",
       "      <td>0.111982</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_U</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0389701</td>\n",
       "      <td>0.193532</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_V</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.397443</td>\n",
       "      <td>0.48939</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_W</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.105254</td>\n",
       "      <td>0.306894</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_Y</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0379262</td>\n",
       "      <td>0.191026</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bldg_class_Z</th>\n",
       "      <td>11496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006611</td>\n",
       "      <td>0.0810423</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>663 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       count unique          top  freq  \\\n",
       "total_area                             11496    NaN          NaN   NaN   \n",
       "structure_completed                    11496    NaN          NaN   NaN   \n",
       "number_structures                      11496    NaN          NaN   NaN   \n",
       "total_gross_area_structures            11496    NaN          NaN   NaN   \n",
       "ratio_building_to_floor_area           11496    NaN          NaN   NaN   \n",
       "allowable_building_to_floor_area       11496    NaN          NaN   NaN   \n",
       "land_use_category                      11496     11           11  4569   \n",
       "number_of_easements                    11496    NaN          NaN   NaN   \n",
       "commercial_floor_area                  11496    NaN          NaN   NaN   \n",
       "residential_floor_area                 11496    NaN          NaN   NaN   \n",
       "office_floor_area                      11496    NaN          NaN   NaN   \n",
       "retail_floor_area                      11496    NaN          NaN   NaN   \n",
       "garage_floor_area                      11496    NaN          NaN   NaN   \n",
       "storage_floor_area                     11496    NaN          NaN   NaN   \n",
       "factory_floor_area                     11496    NaN          NaN   NaN   \n",
       "other_floor_area                       11496    NaN          NaN   NaN   \n",
       "num_floors                             11496    NaN          NaN   NaN   \n",
       "residential_units                      11496    NaN          NaN   NaN   \n",
       "residential_and_non-residential_units  11496    NaN          NaN   NaN   \n",
       "lot_front                              11496    NaN          NaN   NaN   \n",
       "lot_depth                              11496    NaN          NaN   NaN   \n",
       "bldg_front                             11496    NaN          NaN   NaN   \n",
       "bldg_depth                             11496    NaN          NaN   NaN   \n",
       "assess_land                            11496    NaN          NaN   NaN   \n",
       "exempt_land                            11496    NaN          NaN   NaN   \n",
       "exempt_tot                             11496    NaN          NaN   NaN   \n",
       "year_alter_1                           11496    NaN          NaN   NaN   \n",
       "year_alter_2                           11496    NaN          NaN   NaN   \n",
       "structure_completed_2                  11496      2           No  7665   \n",
       "new_zone_dist                          11496      4  Residential  6858   \n",
       "...                                      ...    ...          ...   ...   \n",
       "bsmt_code_0                            11496    NaN          NaN   NaN   \n",
       "bsmt_code_1                            11496    NaN          NaN   NaN   \n",
       "bsmt_code_2                            11496    NaN          NaN   NaN   \n",
       "bsmt_code_4                            11496    NaN          NaN   NaN   \n",
       "bsmt_code_5                            11496    NaN          NaN   NaN   \n",
       "bldg_class_A                           11496    NaN          NaN   NaN   \n",
       "bldg_class_B                           11496    NaN          NaN   NaN   \n",
       "bldg_class_C                           11496    NaN          NaN   NaN   \n",
       "bldg_class_D                           11496    NaN          NaN   NaN   \n",
       "bldg_class_E                           11496    NaN          NaN   NaN   \n",
       "bldg_class_F                           11496    NaN          NaN   NaN   \n",
       "bldg_class_G                           11496    NaN          NaN   NaN   \n",
       "bldg_class_H                           11496    NaN          NaN   NaN   \n",
       "bldg_class_I                           11496    NaN          NaN   NaN   \n",
       "bldg_class_J                           11496    NaN          NaN   NaN   \n",
       "bldg_class_K                           11496    NaN          NaN   NaN   \n",
       "bldg_class_L                           11496    NaN          NaN   NaN   \n",
       "bldg_class_M                           11496    NaN          NaN   NaN   \n",
       "bldg_class_N                           11496    NaN          NaN   NaN   \n",
       "bldg_class_O                           11496    NaN          NaN   NaN   \n",
       "bldg_class_P                           11496    NaN          NaN   NaN   \n",
       "bldg_class_Q                           11496    NaN          NaN   NaN   \n",
       "bldg_class_R                           11496    NaN          NaN   NaN   \n",
       "bldg_class_S                           11496    NaN          NaN   NaN   \n",
       "bldg_class_T                           11496    NaN          NaN   NaN   \n",
       "bldg_class_U                           11496    NaN          NaN   NaN   \n",
       "bldg_class_V                           11496    NaN          NaN   NaN   \n",
       "bldg_class_W                           11496    NaN          NaN   NaN   \n",
       "bldg_class_Y                           11496    NaN          NaN   NaN   \n",
       "bldg_class_Z                           11496    NaN          NaN   NaN   \n",
       "\n",
       "                                              mean          std  min      25%  \\\n",
       "total_area                                  154970  2.40516e+06    0  2008.75   \n",
       "structure_completed                        648.238      917.127    0        0   \n",
       "number_structures                          0.60047      2.52583    0        0   \n",
       "total_gross_area_structures                34745.2       341706    0        0   \n",
       "ratio_building_to_floor_area              0.634766      2.00386    0        0   \n",
       "allowable_building_to_floor_area           1.55182      1.98763    0        0   \n",
       "land_use_category                              NaN          NaN  NaN      NaN   \n",
       "number_of_easements                      0.0473208     0.295869    0        0   \n",
       "commercial_floor_area                      33039.6       337206    0        0   \n",
       "residential_floor_area                     1704.76      47140.6    0        0   \n",
       "office_floor_area                           9664.4      67941.7    0        0   \n",
       "retail_floor_area                          834.764      12928.4    0        0   \n",
       "garage_floor_area                          1766.51      30984.2    0        0   \n",
       "storage_floor_area                         596.883      16081.2    0        0   \n",
       "factory_floor_area                         583.885      15575.6    0        0   \n",
       "other_floor_area                           19450.6       316782    0        0   \n",
       "num_floors                                 1.13944       3.0113    0        0   \n",
       "residential_units                          2.79828      116.382    0        0   \n",
       "residential_and_non-residential_units      4.08968       124.34    0        0   \n",
       "lot_front                                  163.082      371.339    0       25   \n",
       "lot_depth                                   200.47      317.502    0  93.9925   \n",
       "bldg_front                                 39.1312      89.5937    0        0   \n",
       "bldg_depth                                 41.6058      111.604    0        0   \n",
       "assess_land                            1.81569e+06  3.70013e+07    0     2094   \n",
       "exempt_land                            1.67277e+06  3.69734e+07    0        0   \n",
       "exempt_tot                             3.48342e+06   6.5389e+07    0        0   \n",
       "year_alter_1                               203.261      603.037    0        0   \n",
       "year_alter_2                               44.5978      295.527    0        0   \n",
       "structure_completed_2                          NaN          NaN  NaN      NaN   \n",
       "new_zone_dist                                  NaN          NaN  NaN      NaN   \n",
       "...                                            ...          ...  ...      ...   \n",
       "bsmt_code_0                              0.0533229     0.224686    0        0   \n",
       "bsmt_code_1                             0.00730689    0.0851712    0        0   \n",
       "bsmt_code_2                              0.0728079     0.259832    0        0   \n",
       "bsmt_code_4                            0.000173974    0.0131893    0        0   \n",
       "bsmt_code_5                               0.866388     0.340249    0        1   \n",
       "bldg_class_A                            0.00295755    0.0543052    0        0   \n",
       "bldg_class_B                            0.00165275    0.0406222    0        0   \n",
       "bldg_class_C                             0.0221816      0.14728    0        0   \n",
       "bldg_class_D                            0.00339248    0.0581487    0        0   \n",
       "bldg_class_E                            0.00548017    0.0738282    0        0   \n",
       "bldg_class_F                            0.00356646    0.0596158    0        0   \n",
       "bldg_class_G                             0.0453201     0.208014    0        0   \n",
       "bldg_class_H                           0.000434934    0.0208514    0        0   \n",
       "bldg_class_I                             0.0085247    0.0919389    0        0   \n",
       "bldg_class_J                           0.000782881    0.0279703    0        0   \n",
       "bldg_class_K                            0.00678497    0.0820946    0        0   \n",
       "bldg_class_L                            0.00026096    0.0161529    0        0   \n",
       "bldg_class_M                            0.00269659    0.0518609    0        0   \n",
       "bldg_class_N                            0.00452331    0.0671062    0        0   \n",
       "bldg_class_O                             0.0207029     0.142394    0        0   \n",
       "bldg_class_P                             0.0249652     0.156026    0        0   \n",
       "bldg_class_Q                              0.243824     0.429406    0        0   \n",
       "bldg_class_R                           0.000173974    0.0131893    0        0   \n",
       "bldg_class_S                            0.00287056     0.053503    0        0   \n",
       "bldg_class_T                             0.0127001     0.111982    0        0   \n",
       "bldg_class_U                             0.0389701     0.193532    0        0   \n",
       "bldg_class_V                              0.397443      0.48939    0        0   \n",
       "bldg_class_W                              0.105254     0.306894    0        0   \n",
       "bldg_class_Y                             0.0379262     0.191026    0        0   \n",
       "bldg_class_Z                              0.006611    0.0810423    0        0   \n",
       "\n",
       "                                          50%      75%          max  \n",
       "total_area                               6000  35841.5  2.14756e+08  \n",
       "structure_completed                         0     1926         2012  \n",
       "number_structures                           0        1          121  \n",
       "total_gross_area_structures                 0  6063.75     2.76e+07  \n",
       "ratio_building_to_floor_area                0     0.41        59.89  \n",
       "allowable_building_to_floor_area          0.6     2.43           12  \n",
       "land_use_category                         NaN      NaN          NaN  \n",
       "number_of_easements                         0        0            7  \n",
       "commercial_floor_area                       0     3200     2.76e+07  \n",
       "residential_floor_area                      0        0  3.97974e+06  \n",
       "office_floor_area                           0        0  2.68548e+06  \n",
       "retail_floor_area                           0        0       537154  \n",
       "garage_floor_area                           0        0  2.67743e+06  \n",
       "storage_floor_area                          0        0  1.19715e+06  \n",
       "factory_floor_area                          0        0  1.32459e+06  \n",
       "other_floor_area                            0        0     2.76e+07  \n",
       "num_floors                                  0        1           85  \n",
       "residential_units                           0        0         8740  \n",
       "residential_and_non-residential_units       0        1         8750  \n",
       "lot_front                                  60      200         9999  \n",
       "lot_depth                              100.25   200.02         9999  \n",
       "bldg_front                                  0       29      1394.08  \n",
       "bldg_depth                                  0       55         7360  \n",
       "assess_land                             51741   424012  2.85512e+09  \n",
       "exempt_land                                 0   257971  2.85512e+09  \n",
       "exempt_tot                                  0   597375    5.535e+09  \n",
       "year_alter_1                                0        0         2012  \n",
       "year_alter_2                                0        0         2012  \n",
       "structure_completed_2                     NaN      NaN          NaN  \n",
       "new_zone_dist                             NaN      NaN          NaN  \n",
       "...                                       ...      ...          ...  \n",
       "bsmt_code_0                                 0        0            1  \n",
       "bsmt_code_1                                 0        0            1  \n",
       "bsmt_code_2                                 0        0            1  \n",
       "bsmt_code_4                                 0        0            1  \n",
       "bsmt_code_5                                 1        1            1  \n",
       "bldg_class_A                                0        0            1  \n",
       "bldg_class_B                                0        0            1  \n",
       "bldg_class_C                                0        0            1  \n",
       "bldg_class_D                                0        0            1  \n",
       "bldg_class_E                                0        0            1  \n",
       "bldg_class_F                                0        0            1  \n",
       "bldg_class_G                                0        0            1  \n",
       "bldg_class_H                                0        0            1  \n",
       "bldg_class_I                                0        0            1  \n",
       "bldg_class_J                                0        0            1  \n",
       "bldg_class_K                                0        0            1  \n",
       "bldg_class_L                                0        0            1  \n",
       "bldg_class_M                                0        0            1  \n",
       "bldg_class_N                                0        0            1  \n",
       "bldg_class_O                                0        0            1  \n",
       "bldg_class_P                                0        0            1  \n",
       "bldg_class_Q                                0        0            1  \n",
       "bldg_class_R                                0        0            1  \n",
       "bldg_class_S                                0        0            1  \n",
       "bldg_class_T                                0        0            1  \n",
       "bldg_class_U                                0        0            1  \n",
       "bldg_class_V                                0        1            1  \n",
       "bldg_class_W                                0        0            1  \n",
       "bldg_class_Y                                0        0            1  \n",
       "bldg_class_Z                                0        0            1  \n",
       "\n",
       "[663 rows x 11 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe the data\n",
    "nyc_2013.describe(include = \"all\").transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on Reduced Data Set\n",
    "\n",
    "Our data has taken on a completely new shape and contains 3 new features while eliminating 27 less than useful feature.  We now explore our new data to hopefully gain some deeper understanding before delving into classification and/or regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the figure and land use category labels\n",
    "fig, ax = plt.subplots(2,2,figsize=(WIDTH, HEIGHT))\n",
    "labels = ['One & Two Family Buildings', \n",
    "          'Parking Facilities',\n",
    "          'Vacant Land',\n",
    "          'Multi-Family Walk-Up Buildings',\n",
    "          'Multi-Family Elevator Buildings',\n",
    "          'Mixed Residential & Commercial Buildings',\n",
    "          'Commercial & Office Buildings',\n",
    "          'Industrial & Manufacturing',\n",
    "          'Transportation & Utility',\n",
    "          'Public Facilities & Institutions',\n",
    "          'Open Space & Outdoor Recreation']\n",
    "\n",
    "# Top plots\n",
    "plt.suptitle('Zoning Distict & Land Use Category Breakdown')\n",
    "ch1 = nyc_grouped['assess_range'].count().unstack('assess_range')\n",
    "ch1[['low', 'medium', 'high']].plot(kind ='bar', stacked = True , ax = ax[0][0])\n",
    "ax[0][0].set_title(\"Zone District\")\n",
    "\n",
    "nyc1_grouped2 = nyc_2013.groupby([\"land_use_category\", \"assess_range\"])\n",
    "ch2 = nyc1_grouped2['assess_range'].count().unstack('assess_range')\n",
    "ch2[['low', 'medium', 'high']].plot(kind ='bar', stacked=True , ax = ax[0][1])\n",
    "ax[0][1].set_xticklabels((labels))\n",
    "ax[0][1].set_title(\"Land Use Category\")\n",
    "\n",
    "# Bottom plots\n",
    "fig.subplots_adjust(hspace = .6)\n",
    "ch1_grouped = nyc_2013.groupby(['new_zone_dist', 'structure_completed_2'])\n",
    "ch1 = ch1_grouped['structure_completed_2'].count().unstack('structure_completed_2')\n",
    "ch1[['Yes', 'No']].plot(kind = 'bar', stacked=True, ax = ax[1][0])\n",
    "\n",
    "ch2_grouped = nyc_2013.groupby(['land_use_category', 'structure_completed_2'])\n",
    "ch2 = ch2_grouped['structure_completed_2'].count().unstack('structure_completed_2')\n",
    "ch2[['Yes', 'No']].plot(kind = 'bar', stacked=True, ax = ax[1][1])\n",
    "ax[1][1].set_xticklabels((labels))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that can be seen from the above plots are that the zone district plots (left plots) have nearly a uniform ratio between the three different assess value ranges and whether the structure is complete or not.  The Commercial zone appears to be the only code with any noticable deviation.  It kind of makes sense in terms of real estate that commerical property would be more heavily skewed to the expensive and that there is in fact a building front to do business!\n",
    "\n",
    "The plots on the right are a wealth of knowledge.  Looking at the lengths of the bars and the distribution of colors, land use category looks like it would be a prime candidate for a decision tree classifier.  To look more in detail, let's look at some box plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the figure and labels\n",
    "fig, ax = plt.subplots(3,2, figsize=(WIDTH, HEIGHT))\n",
    "plt.suptitle('Boxplot Analysis')\n",
    "land_labels = ['One & Two Family Buildings', \n",
    "          'Multi-Family Walk-Up Buildings'\n",
    "          ,'Multi-Family Elevator Buildings',\n",
    "          'Mixed Residential & Commercial Buildings',\n",
    "          'Commercial & Office Buildings',\n",
    "          'Industrial & Manufacturing',\n",
    "          'Transportation & Utility',\n",
    "          'Public Facilities & Institutions',\n",
    "          'Open Space & Outdoor Recreation',\n",
    "          'Parking Facilities',\n",
    "          'Vacant Land']\n",
    "zone_labels = ['Commercial',\n",
    "               'Manufacturing',\n",
    "               'Park',\n",
    "               'Residential']\n",
    "\n",
    "nyc_2013.loc[nyc_2013.assess_land < 300000].boxplot(\n",
    "    column=['assess_land'],\n",
    "    by='land_use_category',\n",
    "    ax=ax[0][0],\n",
    "    vert=False\n",
    ")\n",
    "ax[0][0].set_yticklabels(land_labels)\n",
    "\n",
    "nyc_2013.loc[nyc_2013.assess_land < 300000].boxplot(\n",
    "    column=['assess_land'],\n",
    "    by='new_zone_dist',\n",
    "    ax=ax[0][1],\n",
    "    vert=False\n",
    ")\n",
    "ax[0][1].set_yticklabels(zone_labels)\n",
    "\n",
    "nyc_2013.loc[nyc_2013.assess_land < 600000].boxplot(\n",
    "    column=['assess_land'],\n",
    "    by='land_use_category',\n",
    "    ax=ax[1][0],\n",
    "    vert=False\n",
    ")\n",
    "ax[1][0].set_yticklabels(land_labels)\n",
    "\n",
    "nyc_2013.loc[nyc_2013.assess_land < 600000].boxplot(\n",
    "    column=['assess_land'],\n",
    "    by='new_zone_dist',\n",
    "    ax=ax[1][1],\n",
    "    vert=False\n",
    ")\n",
    "ax[1][1].set_yticklabels(zone_labels)\n",
    "\n",
    "nyc_2013.loc[nyc_2013.assess_land < 1000000].boxplot(\n",
    "    column=['assess_land'],\n",
    "    by='land_use_category',\n",
    "    ax=ax[2][0],\n",
    "    vert=False\n",
    ")\n",
    "ax[2][0].set_yticklabels(land_labels)\n",
    "\n",
    "nyc_2013.loc[nyc_2013.assess_land < 1000000].boxplot(\n",
    "    column=['assess_land'],\n",
    "    by='new_zone_dist',\n",
    "    ax=ax[2][1],\n",
    "    vert=False\n",
    ")\n",
    "ax[2][1].set_yticklabels(zone_labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above left plot, we can see the zone districts and assessed land value ranges in more detail.  It can be seen that the assess land value of residential and manufacturing districts have a lower assessed value range when compared to parks and commercial zones.  The commerical district has the largest range of assess land values with no outliers outside of the Q3 + 1.5(IQR) range.  We also see that is has a much bigger median compared to the other districts which hold several data points in the upper outlier region (especially Residential zones).  As a result, we can conclude that there is an important relationship between assessed land value range and zone districts.  The Land Use Category plots on the left are a bit harder to interpret since many classes have numerous outliers and some of the data appears to have a median of 0 (Open Space & Outdoor Recreation < $300,000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now one-hot encode Land Use Category and Structure Completed 2 for use in our classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape of data:  (11496, 674)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode \"Land Use Category\" and \"Structure Completed 2\"\n",
    "nyc_2013 = pd.get_dummies(nyc_2013, columns = [\"land_use_category\", \"structure_completed_2\"])\n",
    "    \n",
    "print(\"Final shape of data: \", nyc_2013.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation\n",
    "\n",
    "* **[10 points]** _Choose and explain your evaluation metrics that you will use (i.e., accuracy, precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions._\n",
    "* **[10 points]** _Choose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time._\n",
    "* **[20 points]** _Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!_\n",
    "* **[10 points]** _Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model._\n",
    "* **[10 points]** _Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course._\n",
    "* **[10 points]** _Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Task for New Zone Dist\n",
    "\n",
    "This section will be dedicated to creating, training, and adjusting models designed to classify the New Zone District zoning code. We will models for Logistic Regression, K-Nearest Neighbors (KNN), and XGBoost for classifying this feature.\n",
    "\n",
    "Let's start by examine the distribution of classes and then decide the best split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the distribution of New Zone Dist labels\n",
    "for i in list(range(0,4)):\n",
    "    print(nyc_2013.new_zone_dist.value_counts().index[i] + \": \",\n",
    "          nyc_2013.new_zone_dist.value_counts()[3-i], \n",
    "          \" or\",\n",
    "          round(nyc_2013.new_zone_dist.value_counts()[3-i]\n",
    "                /nyc_2013.new_zone_dist.value_counts().sum()\n",
    "                *100, 2),\n",
    "          \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we see a similar situation where the bulk of the data is labeled in just one category (\"Residential\").  To start the classification task, we need to get our training data in the right format.  That is, the training data must exclude our target variable, New Zone District."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape after one-hot encoding categorical variables:  (11496, 675)\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical variables\n",
    "categorical_fields = list(nyc_2013.drop(\n",
    "    columns = [\"new_zone_dist\"]).select_dtypes(\n",
    "    include=['category']).columns)\n",
    "\n",
    "# Creating dummy variables for their levels\n",
    "X = pd.get_dummies(nyc_2013, columns = categorical_fields).drop(\n",
    "    columns = [\"new_zone_dist\"])\n",
    "    \n",
    "print(\"Training data shape after one-hot encoding categorical variables: \", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model\n",
    "\n",
    "For the Logistic Regression (LR) model, we will adapt some of the findings from the Mini Lab (section 2.1.3) to make this one even better. Accuracy is still a good evaluation metric, but one of the main concerns last time was that the majority of the data fell into 2 of the 11 categories. To prevent the classification model from being too heavily biased in the direction of majority data, we will not employ the 80/20 train/test split that we did in 2.1.3. Instead, we'll try a Statified Split which will maintain the same proportion each label in the testing and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits = 5, test_size = 0.5, train_size=0.5)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "nzd = le.fit_transform(nyc_2013.new_zone_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = nyc_2013.drop(columns = [\"new_zone_dist\"])\n",
    "for trainidx, testidx in cv.split(X, nzd):\n",
    "    # note that these are sparse matrices\n",
    "    X_train = X.iloc[trainidx] \n",
    "    X_test = X.iloc[testidx] \n",
    "    y_train = nzd[trainidx]\n",
    "    y_test = nzd[testidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model = LogisticRegression(solver = 'sag', max_iter = 1000, multi_class= \"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logit_model_result = logit_model.fit(X_train, y_train)\n",
    "test_predictions = logit_model_result.predict(X_test)\n",
    "train_predictions = logit_model_result.predict(X_train)\n",
    "logit_accuracy = accuracy_score(y_test, test_predictions).round(2) * 100\n",
    "print(\"LR Accuracy: \", logit_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the figure\n",
    "fig, ax = plt.subplots(1, 2, figsize=(WIDTH, HEIGHT))\n",
    "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
    "ax[0].set_title('Training Data Results', fontdict={'size': 20})\n",
    "ax[1].set_title('Testing Data Results', fontdict={'size': 20})\n",
    "fig.text(.47, .25, 'New Zone District Predicted label', ha='center', va='center', fontdict={'size': 15})\n",
    "fig.text(-.03, 0.5, 'New Zone District True Label', ha='center', va='center', rotation='vertical', fontdict={'size': 15})\n",
    "# Labels for \"Y\" and \"N\" don't seem to load either.  Tried many different ways.\n",
    "labels = le.classes_\n",
    "print(labels)\n",
    "mat1 = confusion_matrix(y_train, train_predictions)/len(y_train)*100\n",
    "mat2 = confusion_matrix(y_test, test_predictions)/len(y_test)*100\n",
    "ax[0] = sns.heatmap(mat1, \n",
    "            square=True, \n",
    "            annot=True,\n",
    "            fmt='.2g',\n",
    "            cbar_ax = cbar_ax,\n",
    "            linewidths=.5, \n",
    "            ax= ax[0])\n",
    "ax[0].set_xticklabels((labels))\n",
    "ax[0].set_yticklabels((labels))\n",
    "ax[1] = sns.heatmap(mat2, \n",
    "            square=True, \n",
    "            annot=True, \n",
    "            fmt='.2g',\n",
    "            cbar_ax = cbar_ax,\n",
    "            linewidths=.5, \n",
    "            ax= ax[1])\n",
    "ax[1].set_xticklabels((labels))\n",
    "ax[1].set_yticklabels((labels))\n",
    "fig.tight_layout(rect=[0, 0, .9, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each cell in the above confusion matrix represents the percentage of data under the given classification.  We see that the LR model is not very accurate, incorrectly predicting Parks 45% of the time when it should really be Residential.  It seemed to perform best when classifying Residential, with a precision of .7489.\n",
    "\n",
    "Since accuracy is our evaluation metric however, let's see if we can adjust some parameters of the model and obtain a better fit.  The first thing we can try is to increase the maximum number of iterations.  We'll increase it by a factor of 10 and see if this helps at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Increase max_iter to 10,000\n",
    "logit_model = LogisticRegression(solver = 'sag', max_iter = 10000, multi_class= \"multinomial\")\n",
    "\n",
    "# Train the model\n",
    "logit_model_result = logit_model.fit(X_train, y_train)\n",
    "test_predictions = logit_model_result.predict(X_test)\n",
    "train_predictions = logit_model_result.predict(X_train)\n",
    "logit_accuracy = accuracy_score(y_test, test_predictions).round(2) * 100\n",
    "print(\"LR Accuracy: \", logit_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After over 20 minutes of processing, we see that increasing the maximum number of iterations did not do anything to improve the accuracy of the LR model.  Instead, let's try using a different value for C with the original number of iterations (1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Use a low value for C\n",
    "logit_model = LogisticRegression(solver = 'sag', C = .01, max_iter = 1000, multi_class= \"multinomial\")\n",
    "\n",
    "# Train the model\n",
    "logit_model_result = logit_model.fit(X_train, y_train)\n",
    "test_predictions = logit_model_result.predict(X_test)\n",
    "train_predictions = logit_model_result.predict(X_train)\n",
    "logit_accuracy = accuracy_score(y_test, test_predictions).round(2) * 100\n",
    "print(\"LR Accuracy: \", logit_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, a low C value should do a better job fitting the training data.  But when put up against the test data, we see this too had no effect on the overall accuracy.  Next we'll try a high value for C to see if a more \"generalized\" approach is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Use a high value for C\n",
    "logit_model = LogisticRegression(solver = 'sag', C = 100, max_iter = 1000, multi_class= \"multinomial\")\n",
    "\n",
    "# Train the model\n",
    "logit_model_result = logit_model.fit(X_train, y_train)\n",
    "test_predictions = logit_model_result.predict(X_test)\n",
    "train_predictions = logit_model_result.predict(X_train)\n",
    "logit_accuracy = accuracy_score(y_test, test_predictions).round(2) * 100\n",
    "print(\"LR Accuracy: \", logit_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears the value of C does not affect this model.  The last thing to try is a different solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Use the newton-cg model\n",
    "logit_model = LogisticRegression(solver = 'saga', max_iter = 1000, multi_class= \"multinomial\")\n",
    "\n",
    "# Train the model\n",
    "logit_model_result = logit_model.fit(X_train, y_train)\n",
    "test_predictions = logit_model_result.predict(X_test)\n",
    "train_predictions = logit_model_result.predict(X_train)\n",
    "logit_accuracy = accuracy_score(y_test, test_predictions).round(2) * 100\n",
    "print(\"LR Accuracy: \", logit_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saga solver fails to deliver any improvement to test data accuracy.  We'll try Newton's Method as a last resort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Accuracy:  69.0\n",
      "CPU times: user 34min 34s, sys: 17 s, total: 34min 51s\n",
      "Wall time: 5min 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/austinsimeone/anaconda3/envs/env_7331/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Use the newton-cg model\n",
    "logit_model = LogisticRegression(solver = 'newton-cg', max_iter = 1000, multi_class= \"multinomial\")\n",
    "\n",
    "# Train the model\n",
    "logit_model_result = logit_model.fit(X_train, y_train)\n",
    "test_predictions = logit_model_result.predict(X_test)\n",
    "train_predictions = logit_model_result.predict(X_train)\n",
    "logit_accuracy = accuracy_score(y_test, test_predictions).round(2) * 100\n",
    "print(\"LR Accuracy: \", logit_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the LR model does not discriminate against the maximum number of iterations or the value of C when determining accuracy.  Using the Newton-cg solver, however, seems to have nearly doubled the accuracy.  We'll examine the confusion matrix below to gain some insight on the classification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(WIDTH, HEIGHT))\n",
    "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
    "ax[0].set_title('Training Data Results', fontdict={'size': 20})\n",
    "ax[1].set_title('Testing Data Results', fontdict={'size': 20})\n",
    "fig.text(.47, .25, 'New Zone District Predicted label', ha='center', va='center', fontdict={'size': 15})\n",
    "fig.text(-.03, 0.5, 'New Zone District True Label', ha='center', va='center', rotation='vertical', fontdict={'size': 15})\n",
    "# Labels for \"Y\" and \"N\" don't seem to load either.  Tried many different ways.\n",
    "labels = le.classes_\n",
    "print(labels)\n",
    "mat1 = confusion_matrix(y_train, train_predictions)/len(y_train)*100\n",
    "mat2 = confusion_matrix(y_test, test_predictions)/len(y_test)*100\n",
    "ax[0] = sns.heatmap(mat1, \n",
    "            square=True, \n",
    "            annot=True,\n",
    "            fmt='.2g',\n",
    "            cbar_ax = cbar_ax,\n",
    "            linewidths=.5, \n",
    "            ax= ax[0])\n",
    "ax[0].set_xticklabels((labels))\n",
    "ax[0].set_yticklabels((labels))\n",
    "ax[1] = sns.heatmap(mat2, \n",
    "            square=True, \n",
    "            annot=True, \n",
    "            fmt='.2g',\n",
    "            cbar_ax = cbar_ax,\n",
    "            linewidths=.5, \n",
    "            ax= ax[1])\n",
    "ax[1].set_xticklabels((labels))\n",
    "ax[1].set_yticklabels((labels))\n",
    "fig.tight_layout(rect=[0, 0, .9, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the model now correctly classifies the majority of the data as Residential.  The precision for the Residential classification is only .6818, which is actually a step down from the first model we ran (.7489). Depending on the application of classifying, the original model might be better. But seeing as how accuracy is our metric for evaluating the model, we conclude that the LR model performs best with the Newton-cg solver.\n",
    "\n",
    "The main advantage to the LR model is that there are several parameters to experiment with to find the best model.  Unfortunately for our data, many of those did not seem to have a large impact on the accuracy of the model.  Newton's Method performed significantly better than Stochasitc Average Gradient (SAG) Descent and its variant, SAGA, but the long processing time makes it cumbersome to vary additionaly parameters to truly achieve the best model possible.  There are no doubt other LR models that will produce a higher accuracy if you're willing to wait approximately 10 minutes per trial.  Instead, we will explore other classification models.  But before we move on, let's see if we can understand which features carried the most weight in final LR model we produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights for all features\n",
    "impt_features = logit_model_result.coef_\n",
    "\n",
    "# Create a placeholder for just the top 10 most important\n",
    "top_impt = np.zeros( (impt_features.shape[0], 10), dtype = 'float')\n",
    "indices = np.zeros( (impt_features.shape[0], 10), dtype = 'int')\n",
    "\n",
    "# Define the figure\n",
    "fig, ax = plt.subplots(2, 2, figsize=(WIDTH, HEIGHT))\n",
    "\n",
    "# Extract the relevant data\n",
    "for i in range(0, impt_features.shape[0]):\n",
    "    indices[i,:] = impt_features[i, :].argsort()[:10]\n",
    "    top_impt[i, :] = impt_features[i, indices[i, :]]\n",
    "    #plt.xticks(range(0,10), list( X_train.columns[i] for i in impt_features[i, :].argsort()[:10]), rotation = 90)\n",
    "\n",
    "# Plot it on a subplot\n",
    "ax[0, 0].set_title('Commercial Features', fontdict={'size': 20})\n",
    "ax[0, 0].bar(range(0, 10), top_impt[0, :])\n",
    "ax[0, 0].xaxis.set_ticks(np.arange(0, 10))\n",
    "ax[0, 0].set_xticklabels( list( X_train.columns[i] for i in indices[0, :]), rotation = 90)\n",
    "ax[0, 1].set_title('Manufacturing Features', fontdict={'size': 20})\n",
    "ax[0, 1].bar(range(len(top_impt[1, :])), top_impt[1, :])\n",
    "ax[0, 1].xaxis.set_ticks(np.arange(0, 10))\n",
    "ax[0, 1].set_xticklabels( list( X_train.columns[i] for i in indices[1, :]), rotation = 90)\n",
    "fig.subplots_adjust(hspace = .6)\n",
    "ax[1, 0].set_title('Park Features', fontdict={'size': 20})\n",
    "ax[1, 0].bar(range(len(top_impt[2, :])), top_impt[2, :])\n",
    "ax[1, 0].xaxis.set_ticks(np.arange(0, 10))\n",
    "ax[1, 0].set_xticklabels( list( X_train.columns[i] for i in indices[2, :]), rotation = 90)\n",
    "ax[1, 1].set_title('Residential Features', fontdict={'size': 20})\n",
    "ax[1, 1].bar(range(len(top_impt[3, :])), top_impt[3, :])\n",
    "ax[1, 1].xaxis.set_ticks(np.arange(0, 10))\n",
    "ax[1, 1].set_xticklabels( list( X_train.columns[i] for i in indices[3, :]), rotation = 90)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see the weights of the top 10 most influential features in determining the given classification.  Exempt Land, Total Area, Commercial Floor Area, Residential Floor Area, and Total Gross Area Structures make appearances in 3 of the 4 classifications.  The actual values of the weights are output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = (list( X_train.columns[i] for i in indices[0, :]),\n",
    "list( X_train.columns[i] for i in indices[1, :]),\n",
    "list( X_train.columns[i] for i in indices[2, :]),\n",
    "list( X_train.columns[i] for i in indices[3, :]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in lst for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(flat_list).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_weights = pd.DataFrame(top_impt, \n",
    "                          index = [\"Commercial\", \"Manufacturing\", \"Park\", \"Residential\"],\n",
    "                          columns = [\"Rank 1\", \"Rank 2\", \"Rank 3\", \"Rank 4\", \"Rank 5\", \n",
    "                                     \"Rank 6\", \"Rank 7\", \"Rank 8\", \"Rank 9\", \"Rank 10\"])\n",
    "LR_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Residential, it is clear to see why Assessed Land Value carries so much weight.  Housing in NYC is unaffordable to most for a reason after all!  Likewise, it makes sense that when classifying a zone as a Park, you would want to look at the Exempt Total since Parks are unlikely to pay hefty taxes.  Similarly, if a lot has a small Total Area, I would lean to believe it to my a Commercial store front.  What is curious to find is the Bldg Class Q that occupies the rank 8 spot in Residential.  This is odd becuase Bldg Class code Q is for Outdoor Recreation Facilities.  Perhaps the model is trying to rule out Park by checking for this Bulding Class code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Model\n",
    "\n",
    "For the KNN model to predict New Zone Distist, we will use accuracy and precision as our evaluation metric since we have four different classes and we would like to know the accuracy per class.  We will use the Statified 50/50 Split which will maintain the same proportion of each label in the testing and training data.  This will ensure that there isn't a biased distribution of labels from one classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the split\n",
    "cv = StratifiedShuffleSplit(n_splits = 5, test_size = 0.5, train_size=0.5)\n",
    "\n",
    "# Encode the labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "nzd = le.fit_transform(nyc_2013.new_zone_dist)\n",
    "\n",
    "# Split the data\n",
    "for trainidx, testidx in cv.split(X, nzd):\n",
    "    # note that these are sparse matrices\n",
    "    X_train = X.iloc[trainidx] \n",
    "    X_test = X.iloc[testidx] \n",
    "    y_train = nzd[trainidx]\n",
    "    y_test = nzd[testidx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common practice is to set K equal to the square root of the number of training samples for best results.  We'll test the KNN model for every value of K from 1 to $\\sqrt n$ + 3 to find the K which gives us the best accuracy, where _n_ is the total number of training samples, 5,748.  The additional 3 beyond $\\sqrt n$ is just to see if further values yield a better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "KNNdf = pd.DataFrame(index = list(range(1, int(sqrt(X_train.shape[0]) + 3))),\n",
    "                     columns = [\"Accuracy\"],\n",
    "                     dtype = \"float\")\n",
    "for i in range(1, len(KNNdf) + 1 ):\n",
    "    clf = KNeighborsClassifier(n_neighbors = i)\n",
    "    clf.fit(X_train, y_train)\n",
    "    yhat= clf.predict(X_test)\n",
    "    total_accuracy = mt.accuracy_score(y_test, yhat) * 100\n",
    "    KNNdf.loc[i, \"Accuracy\"] = total_accuracy\n",
    "\n",
    "print(\"Max accuracy is\", \n",
    "      round(KNNdf.loc[KNNdf.Accuracy.idxmax(), \"Accuracy\"], 2), \n",
    "      \"with\", KNNdf.Accuracy.idxmax(), \"neighbors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the optimal number of neighbors is 48.  Had we used the $\\sqrt n$ approach, we would have seen an accuracy of 66.13%, or a difference of 0.55%.  Let's now explore a few other paremters to tune using K = 48 $\\pm$ 2.  The other parameters we'll look at are the algorithm types: \"Brute Force, K-D Tree, and Ball Tree;\" and leaf size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "KNNdfs = pd.DataFrame(index = list( range(46, 50)),\n",
    "                      columns = [\"brute\", \"kd_tree\", \"ball_tree\"],\n",
    "                      dtype = \"float\")\n",
    "\n",
    "# Iterate through K = 48 +/- 2\n",
    "for K in list(range(46, 51)):\n",
    "    # Then iterate through each algorithm\n",
    "    for algorithm in list(KNNdfs.columns):\n",
    "        clf = KNeighborsClassifier(n_neighbors = K, algorithm = algorithm)\n",
    "        clf.fit(X_train, y_train)\n",
    "        yhat= clf.predict(X_test)\n",
    "        accuracy = mt.accuracy_score(y_test, yhat) * 100\n",
    "        KNNdfs.loc[K, algorithm] = accuracy\n",
    "\n",
    "# Output the results\n",
    "print(KNNdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see is that the highest accuracy comes from 48 neighbors regardless of the type of algorithm we use.  Before we say every method is the same however, let's take a moment to explore the leaf size for KD-Tree and Ball Tree algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Make a new multiIndex dataframe\n",
    "arrays = [[46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46,\n",
    "           47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
    "           48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48,\n",
    "           49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49,\n",
    "           50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50],\n",
    "          [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
    "           25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
    "           25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
    "           25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
    "           25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]]\n",
    "KNNdfs = pd.DataFrame(index = arrays,\n",
    "                      columns = [\"kd_tree\", \"ball_tree\"],\n",
    "                      dtype = \"float\")\n",
    "\n",
    "# Iterate through each K value\n",
    "for K in list( range(46, 51)):\n",
    "    # Iterate through each leaf size\n",
    "    for leaves in list(range( 25, 36)):\n",
    "        # Then iterate through each algorithm\n",
    "        for algorithm in list(KNNdfs.columns):\n",
    "            clf = KNeighborsClassifier(n_neighbors = K, leaf_size = leaves, algorithm = algorithm)\n",
    "            clf.fit(X_train, y_train)\n",
    "            yhat= clf.predict(X_test)\n",
    "            accuracy = mt.accuracy_score(y_test, yhat) * 100\n",
    "            KNNdfs.loc[(K,  leaves), algorithm] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KD Tree best K: \", KNNdfs.kd_tree.idxmax()[0], \n",
    "      \"\\nBest leaf size: \", KNNdfs.kd_tree.idxmax()[1],\n",
    "      \"\\nBest accuracy: \", round(KNNdfs.loc[KNNdfs.kd_tree.idxmax(), \"kd_tree\"], 2))\n",
    "\n",
    "print(\"\\nBall Tree best K: \", KNNdfs.ball_tree.idxmax()[0], \n",
    "      \"\\nBest leaf size: \", KNNdfs.ball_tree.idxmax()[1],\n",
    "      \"\\nBest accuracy: \", round(KNNdfs.loc[KNNdfs.ball_tree.idxmax(), \"ball_tree\"], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best K value is still 48, but KD Tree and Ball Tree appear to be at a tie for accuracy with a leaf size of 25.  We'll restrict the model to just K = 48 now and test smaller leaf sizes to see if it makes any difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Make a new multiIndex dataframe\n",
    "KNNdfs = pd.DataFrame(index = list(range( 1, 26)),\n",
    "                      columns = [\"kd_tree\", \"ball_tree\"],\n",
    "                      dtype = \"float\")\n",
    "\n",
    "# Iterate through every leaf size\n",
    "for leaves in list(range( 1, 26)):\n",
    "    # Then iterate through each algorithm\n",
    "    for algorithm in list(KNNdfs.columns):\n",
    "        clf = KNeighborsClassifier(n_neighbors = 48, leaf_size = leaves, algorithm = algorithm)\n",
    "        clf.fit(X_train, y_train)\n",
    "        yhat= clf.predict(X_test)\n",
    "        accuracy = mt.accuracy_score(y_test, yhat) * 100\n",
    "        KNNdfs.loc[leaves, algorithm] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KD Tree best leaf size: \", KNNdfs.kd_tree.idxmax(),\n",
    "      \"\\nBest accuracy: \", round(KNNdfs.loc[KNNdfs.kd_tree.idxmax(), \"kd_tree\"], 2))\n",
    "\n",
    "print(\"\\nBall Tree best leaf size: \", KNNdfs.ball_tree.idxmax(),\n",
    "      \"\\nBest accuracy: \", round(KNNdfs.loc[KNNdfs.ball_tree.idxmax(), \"ball_tree\"], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the KNN model performs the same under the Brute Force algorithm, KD Tree, or Ball Tree and is unaffected by the leaf size.  It would seem the only parameter that makes any signficant difference is the value for K, and even then, the difference is marginal compared to other K values.  \n",
    "\n",
    "Nonetheless, our KNN model will use 48 neighbors, 1 for the leaf size, and the algorithm set to the default value, 'auto.'  Using these values, we will now examine the precision of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters as described above\n",
    "clf=KNeighborsClassifier(n_neighbors = 48, leaf_size = 1, algorithm = 'auto')\n",
    "clf.fit(X_train, y_train)\n",
    "yhat= clf.predict(X_test)\n",
    "train_predt= clf.predict(X_train)\n",
    "\n",
    "# Define some helper functions\n",
    "def per_class_accuracy(ytrue, yhat):\n",
    "    conf = mt.confusion_matrix(ytrue, yhat)\n",
    "    norm_conf = conf.astype('float') / conf.sum(axis = 1)[:, np.newaxis]\n",
    "    return np.diag(norm_conf)\n",
    "\n",
    "def plot_class_acc(ytrue, yhat, title = ''):\n",
    "    acc_list = per_class_accuracy(ytrue, yhat)\n",
    "    plt.bar(range(len(acc_list)), acc_list)\n",
    "    plt.xlabel('Classification')\n",
    "    plt.ylabel('Accuracy within class')\n",
    "    plt.title(title + \", Total Acc = %.1f\"%(100 * mt.accuracy_score(ytrue, yhat)))\n",
    "    plt.grid()\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xticks([0, 1, 2 ,3], [\"Commercial\", \"Manufacturing\", \"Park\", \"Residential\"])\n",
    "    plt.show()\n",
    "    \n",
    "# Display the plot\n",
    "plot_class_acc(y_test, yhat, title=\"KNN model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this model is especially good at classifying Residential zoning districts and not so great at classifying Commercial or Manufacturing districts.  Our favorite confusion matrix figure below illustrates the distribution of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(WIDTH, HEIGHT))\n",
    "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
    "ax[0].set_title('Training Data Results', fontdict={'size': 20})\n",
    "ax[1].set_title('Testing Data Results', fontdict={'size': 20})\n",
    "fig.text(.47, .25, 'New Zone District Predicted label', ha='center', va='center', fontdict={'size': 15})\n",
    "fig.text(-.03, 0.5, 'New Zone District True Label', ha='center', va='center', rotation='vertical', fontdict={'size': 15})\n",
    "# Labels for \"Y\" and \"N\" don't seem to load either.  Tried many different ways.\n",
    "labels = le.classes_\n",
    "print(labels)\n",
    "mat1 = confusion_matrix(y_train, train_predt)/len(y_train)*100\n",
    "mat2 = confusion_matrix(y_test, yhat)/len(y_test)*100\n",
    "ax[0] = sns.heatmap(mat1, \n",
    "            square=True, \n",
    "            annot=True,\n",
    "            fmt='.2g',\n",
    "            cbar_ax = cbar_ax,\n",
    "            linewidths=.5, \n",
    "            ax= ax[0])\n",
    "ax[0].set_xticklabels((labels))\n",
    "ax[0].set_yticklabels((labels))\n",
    "ax[1] = sns.heatmap(mat2, \n",
    "            square=True, \n",
    "            annot=True, \n",
    "            fmt='.2g',\n",
    "            cbar_ax = cbar_ax,\n",
    "            linewidths=.5, \n",
    "            ax= ax[1])\n",
    "ax[1].set_xticklabels((labels))\n",
    "ax[1].set_yticklabels((labels))\n",
    "fig.tight_layout(rect=[0, 0, .9, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the model appears to be heavily biased into predicting Residential zone districts versus any other classification.  This is likely due to the shear volume of observations that have this classification.  Although we retain the same percentage of each label in the stratified shuffle split, there are still many many more instances of residentially zoned lots.  The KNN model at its best produced a precision of just over .72 for the Residential classification.  Parks were just under .53 precision.\n",
    "\n",
    "One of the key advantages of the KNN model is its flexibility.  We explored different K values using different algorithms and leaf size.  The unfortunate down side of our data is that these other parameters did not seem to make a difference in the evaluation metric, accuracy, we were interested in.\n",
    "\n",
    "Interpreting feature importance of a KNN model is not intuitive.  Since an observation is classified according to the distance metric of the K nearest neighbors, there is no obvious way to extract this information in the case of a specific class.  We could however, examine the classifications for the K nearest neighbors around a specific observation, but that provides little insight to the importance of individual features.\n",
    "\n",
    "We leave the KNN model for now and look towards XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBooster Model\n",
    "\n",
    "The open-source XGBoost algorithym was developed by Tianqi Chen. It has become a widely used tool among Data Scientists.  Its name stands for eXtreme Gradient Boosting; and it is capable of performing the three main forms of gradient boosting (Gradient Boosting (GB), Stochastic GB and Regularized GB).\n",
    "\n",
    "Boosting is an ensemble method which creates a strong classifier based on \"weak\" classifiers.  Weak and strong refer to a how correlated are the learners to a target variable, meaning that the errors of the previous model are corrected by the next predictor, iteratively speaking. As opposed to AdaBoost, where a weight is added to the weak learners after iteration, XGBoost uses gradient descent, i.e., the method fits a new model to the new residuals of the previous prediction.\n",
    "\n",
    "Our XGBooster model will use the same split that we leveraged earlier via the Stratified Shuffle Split function from sklearn.  The parameters we used are: 5 split iterations utilizing 50% of the labels from each category to maintain an apples to apples comparison between the models.  Later, we'll modify the split between test and training data to 20% and 80% and raise the splitting iterations to 10 to see how this will impact the performance of the model.  Once again, we'll be examining the model's accuracy and precision as our evaluation metrics.  This way we have something to compare against the previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cv = StratifiedShuffleSplit(n_splits = 5, test_size = 0.5, train_size=0.5)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "nzd = le.fit_transform(nyc_2013.new_zone_dist)\n",
    "\n",
    "for trainidx, testidx in cv.split(X, nzd ):\n",
    "    X_train  = X.iloc[trainidx] \n",
    "    X_test   = X.iloc[testidx] \n",
    "    y_train  =  nzd[trainidx]\n",
    "    y_test   =  nzd[testidx]\n",
    "\n",
    "\n",
    "our_xgbooster = xgb.XGBClassifier(max_depth = 5)\n",
    "our_xgbooster.fit(X_train, y_train)\n",
    "strong_accuracy_test = our_xgbooster.score(X_test, y_test)\n",
    "print(\"XGBooster Strong Accuracy at 50/50 CV split: \", strong_accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cv = StratifiedShuffleSplit(n_splits = 10, test_size = 0.2, train_size=0.8)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "nzd = le.fit_transform(nyc_2013.new_zone_dist)\n",
    "\n",
    "for trainidx, testidx in cv.split(X, nzd ):\n",
    "    X_train  = X.iloc[trainidx] \n",
    "    X_test   = X.iloc[testidx] \n",
    "    y_train  =  nzd[trainidx]\n",
    "    y_test   =  nzd[testidx]\n",
    "\n",
    "\n",
    "our_xgbooster = xgb.XGBClassifier(max_depth = 5)\n",
    "our_xgbooster.fit(X_train, y_train)\n",
    "strong_accuracy_test = our_xgbooster.score(X_test, y_test)\n",
    "print(\"XGBooster Strong Accuracy at 20/80 CV split:\", strong_accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, using the 80/20 split improved the accuracy of the model from 95.96% to 96.13, but the execution turn-around-time (TAT) incremented about 1 minute from 2 to 3.5 minutes.  It does beg the question if we should have been using 80/20 for all of our previous models, or if this slight increase in accuracy is specific to the XGBoost model.\n",
    "\n",
    "We also reduced the parameter max_depth to 5 from the default which is 6 to control the maximum depth of a tree and avoid overfitting.  We tested the XGBooster model with a max_depth of 7 to gauge the performance and accuracy when raising this parameter value; accuracy was reduced and TAT went up to 4+ minutes (code not shown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBooster Predictions\n",
    "XGBooster_test_predictions  = pd.DataFrame(our_xgbooster.predict(X_test))\n",
    "XGBooster_train_predictions = pd.DataFrame(our_xgbooster.predict(X_train))\n",
    "\n",
    "# Define the figure\n",
    "fig, ax = plt.subplots(1, 2, figsize=(WIDTH, HEIGHT))\n",
    "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
    "ax[0].set_title('XGB Training Data Results', fontdict={'size': 20})\n",
    "ax[1].set_title('XGB Testing Data Results', fontdict={'size': 20})\n",
    "fig.text(.47, .25, \n",
    "         'New Zone District Predicted label', \n",
    "         ha = 'center', \n",
    "         va = 'center',\n",
    "         fontdict = {'size': 15})\n",
    "fig.text(-.03, 0.5, \n",
    "         'New Zone District True Label',\n",
    "         ha = 'center',\n",
    "         va = 'center',\n",
    "         rotation = 'vertical',\n",
    "         fontdict = {'size': 15})\n",
    "labels = le.classes_\n",
    "mat1 = confusion_matrix(y_train, XGBooster_train_predictions) / len(y_train) * 100\n",
    "mat2 = confusion_matrix(y_test, XGBooster_test_predictions) / len(y_test) * 100\n",
    "ax[0] = sns.heatmap(mat1, \n",
    "            square=True, \n",
    "            annot=True,\n",
    "            fmt='.2g',\n",
    "            cbar_ax = cbar_ax,\n",
    "            linewidths=.5, \n",
    "            ax= ax[0])\n",
    "ax[0].set_xticklabels((labels))\n",
    "ax[0].set_yticklabels((labels))\n",
    "\n",
    "ax[1] = sns.heatmap(mat2, \n",
    "            square = True, \n",
    "            annot = True, \n",
    "            fmt = '.2g',\n",
    "            cbar_ax = cbar_ax,\n",
    "            linewidths = .5, \n",
    "            ax = ax[1])\n",
    "ax[1].set_xticklabels((labels))\n",
    "ax[1].set_yticklabels((labels))\n",
    "fig.tight_layout(rect=[0, 0, .9, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our XGBooster model performance for classifying New Zone District was highly accurate with a accuracy score of 96.13%. It is worth mentioning that the model took a significantly shorter time to achieve this level of accuracy when compared to Logistic Regression and KNN.\n",
    "\n",
    "The confusion matrices of the predictions of the testing and training data are shown above.  In section 3.2.1 we calculated the distribution of the New Zone District feature in the data set that had the following results:\n",
    "\n",
    "1. Residential:  6858  or 59.66 %\n",
    "2. Park:  2551  or 22.19 %\n",
    "3. Manufacturing:  1281  or 11.14 %\n",
    "4. Commercial:  806  or 7.01 %\n",
    "\n",
    "Comparing the results to the confussion matrix's results on our testing data, we see that the results are very close to the original classification in the data set, as follows:\n",
    "\n",
    "1. Residential:    59 %\n",
    "2. Park:           22 %\n",
    "3. Manufacturing:  11 %\n",
    "4. Commercial:     4.4 %\n",
    "\n",
    "Clearly, the XGBoost model is superior to LR and KNN for our data set.  Couple that with the simple fact that it processed so quickly makes it an obvious choice for future models.  We only explored one parameter in the XGBoost model (max depth), but we're confident that we can delve into all of the optional parameters and find an even better fit given more time.  Because of these highly accurate results, we will examine XGBoost in the next classification task as well for Assessed Land Value Range.  Before we get there though, let's look at feature importance for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the weights for all features\n",
    "impt_features = our_xgbooster.feature_importances_\n",
    "\n",
    "# The top 10 features are where impt_features are sorted in descending order\n",
    "fig = plt.figure(figsize = (WIDTH, HEIGHT))\n",
    "top_impt = impt_features[impt_features.argsort()[-10:][::-1]]\n",
    "plt.bar(range(len(top_impt)), top_impt)\n",
    "plt.xticks(range(0,10), list( X_train.columns[i] for i in impt_features.argsort()[-10:][::-1]), rotation = 90)\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see how just the top 10 features weigh in on determining the classification for the XGBoost model.  At rank 1 we see Land Use Category 9: Open Space & Outdoor Recreation, followed by Building Class Q: Outdoor Recreation Facilities.  Combined, these two features can split the data to confidently classify lots which are parks and which are not.  Further down we see Land Use Category 5: Commercial & Office Buildings.  It seems as though the Allowable Building to Floor Area would then be used to separate Commercial from Residential, and Land Use Category 5 solidifies the decision.  It's curious to see two Fire Companies listed in the top 10.  Perhaps they cover a large area, devoid of diversity in zoning districts?  In any case, we see the numerical value of the top 10 weights below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XG_weights = pd.DataFrame(top_impt, \n",
    "                          index = list( X_train.columns[i] for i in impt_features.argsort()[-10:][::-1]),\n",
    "                          columns = [\"Weight\"])\n",
    "\n",
    "XG_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 2 features are orders of magnitude greater than the rest with the exception of Allowable Building to Floor Area (rank 3).  These numbers, in conjunction with the bar chart above illustrate just how important those features are compared to the rest.  At a 96% accuracy, this model has done the best job so far interpreting feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two models that can derive variable importance only had one variable in common and that was land_use_category_9. This tells us that based on the model that we choose, we will need to find different methods to apply it out in the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Task for Assessed Land Value Range\n",
    "\n",
    "This portion is concerned with the creation and manipulation of models designed to classify the Assessed Land Value Range feature.  For this attribute, we will build models using Random Forest, Naive Bayes, and XGBoost once again.  The first step however, is to get our training data into a usable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the distribution of Assess Land Value labels\n",
    "for i in list(range(0,3)):\n",
    "    print(nyc_2013.assess_range.value_counts().index[i] + \": \",\n",
    "          nyc_2013.assess_range.value_counts()[2-i], \n",
    "          \" or\",\n",
    "          round(nyc_2013.assess_range.value_counts()[2-i]\n",
    "                /nyc_2013.assess_range.value_counts().sum()\n",
    "                *100, 2),\n",
    "          \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is much more evenly spread across the 3 levels in the feature. This bodes well for modeling purposes. We now split the data into testing and training as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical variables.  Assessed Land Value must also be excluded since it completely determines the\n",
    "# target variable Assessed Land Value Range\n",
    "categorical_fields = list(nyc_2013.drop(\n",
    "    columns = [\"assess_range\"]).select_dtypes(\n",
    "    include=['category']).columns)\n",
    "\n",
    "# Creating dummy variables for their levels\n",
    "X = pd.get_dummies(nyc_2013, columns = categorical_fields).drop(\n",
    "    columns = [\"assess_range\", \"assess_land\"])\n",
    "    \n",
    "print(\"Shape after one-hot encoding categorical variables: \", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model\n",
    "\n",
    "The Random Forest (RF) model will use the same split as before (Stratified Shuffle Split) with 5 splits utilizing 50% of the labels from each category.  Accuracy and precision will be our evaluation metrics.  We expect RF to perform very well in both of these areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits = 5, test_size = 0.5, train_size=0.5)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "alr = le.fit_transform(nyc_2013.assess_range)\n",
    "\n",
    "for trainidx, testidx in cv.split(X, nzd):\n",
    "    # note that these are sparse matrices\n",
    "    X_train = X.iloc[trainidx] \n",
    "    X_test = X.iloc[testidx] \n",
    "    y_train = alr[trainidx]\n",
    "    y_test = alr[testidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf=RandomForestClassifier(max_depth=50, n_estimators=150, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "yhat= clf.predict(X_test)\n",
    "total_accuracy = mt.accuracy_score(y_test, yhat)\n",
    "print ('Accuracy', total_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incredibly, on the first run the accuracy of this model was over 90% and computed in under 2 seconds!  It may be prudent to find the accuracy ratio per class of the Assessed Land Value Range.  High accuracy and low false positive ratios for each class are what make this model a good fit considering slightly unbalanced data between the different number of observation per level in this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class_accuracy(ytrue, yhat):\n",
    "    conf = mt.confusion_matrix(ytrue, yhat)\n",
    "    norm_conf = conf.astype('float') / conf.sum(axis = 1)[:, np.newaxis]\n",
    "    return np.diag(norm_conf)\n",
    "\n",
    "def plot_class_acc(ytrue, yhat, title=''):\n",
    "    acc_list = per_class_accuracy(ytrue, yhat)\n",
    "    plt.bar(range(len(acc_list)), acc_list)\n",
    "    plt.xlabel('Classification')\n",
    "    plt.ylabel('Accuracy within class')\n",
    "    plt.title(title + \", Total Acc = %.1f\"%(100*mt.accuracy_score(ytrue, yhat)))\n",
    "    plt.grid()\n",
    "    plt.ylim([0,1])\n",
    "    plt.xticks([0, 1, 2], [\"Low\", \"Medium\", \"High\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_class_acc(y_test, yhat, title = \"Random Forest, Raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the accuracy of every classification is high, but we see that the \"high\" classification has the lowest accuracy of the 3.  This may be due to the simple fact that this label had the most number of observations between the other two.\n",
    "\n",
    "We now examine our familiar confusion matrix to understand the individual predictions better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predt= clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the figure\n",
    "fig, ax = plt.subplots(1, 2, figsize=(WIDTH, HEIGHT))\n",
    "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
    "ax[0].set_title('Training Data Results', fontdict={'size': 20})\n",
    "ax[1].set_title('Testing Data Results', fontdict={'size': 20})\n",
    "fig.text(.47, .25, 'Assess Range Predicted label', ha='center', va='center', fontdict={'size': 15})\n",
    "fig.text(-.03, 0.5, 'Assess Range True Label', ha='center', va='center', rotation='vertical', fontdict={'size': 15})\n",
    "# Labels for \"Y\" and \"N\" don't seem to load either.  Tried many different ways.\n",
    "labels = le.classes_\n",
    "mat1 = confusion_matrix(y_train, train_predt)/len(y_train)*100\n",
    "mat2 = confusion_matrix(y_test, yhat)/len(y_test)*100\n",
    "ax[0] = sns.heatmap(mat1, \n",
    "            square=True, \n",
    "            annot=True,\n",
    "            fmt='.2g',\n",
    "            cbar_ax = cbar_ax,\n",
    "            linewidths=.5, \n",
    "            ax= ax[0])\n",
    "ax[0].set_xticklabels((labels))\n",
    "ax[0].set_yticklabels((labels))\n",
    "ax[1] = sns.heatmap(mat2, \n",
    "            square=True, \n",
    "            annot=True, \n",
    "            fmt='.2g',\n",
    "            cbar_ax = cbar_ax,\n",
    "            linewidths=.5, \n",
    "            ax= ax[1])\n",
    "ax[1].set_xticklabels((labels))\n",
    "ax[1].set_yticklabels((labels))\n",
    "fig.tight_layout(rect=[0, 0, .9, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training set, 100% precision is obtained for the \"High\" classification, indicating the data may be overfitted.  But when we examine the test data, the model doesn't perform quite as well as in the training data, lending credence to the idea that the data may be overfitted. Again, the overall accuracy for the test set was 90.27%.\n",
    "\n",
    "Next, let's see if we can identify which features were most important in classifying the Assessed Land Value Range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# out of 2023 features, there must be some unimportnat features for bulding final model, therefore the most\n",
    "# improtant features can beindentified using the following histigram according to percent of importance.\n",
    "impt_features = clf.feature_importances_\n",
    "top_impt = impt_features[impt_features.argsort()[-10:][::-1]]\n",
    "plt.bar(range(len(top_impt)), top_impt)\n",
    "plt.xticks(range(0,10), list( X_train.columns[i] for i in impt_features.argsort()[-10:][::-1]), rotation = 90)\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unspurprising, the single most important feature is the Total Area with Exempt Land, Lot Front, and Exempt Total following closely behind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_weights = pd.DataFrame(top_impt, \n",
    "                          index = [\"Rank 1\", \"Rank 2\", \"Rank 3\", \"Rank 4\", \"Rank 5\", \n",
    "                                     \"Rank 6\", \"Rank 7\", \"Rank 8\", \"Rank 9\", \"Rank 10\"],\n",
    "                          columns = [\"Weight\"])\n",
    "RF_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to classify whether a given lot is going to fall into a particular range scale (Low, Medium, or High), it is reasonable to believe that features pertaining to the area of the land how much of it is taxed would be the greatest indicators for how to classify the lot.  We also Commercial Floor Area in the top 10 of important features analyzed when making this classification.  We know from our Data Preparation phase that commercially zone districts hold the largest percentage of \"High\" valued Assessed Land Range (greater than 66%) compared to the other zoning regions.  If a lot then is small, the model appears to placing some weight as to how much of the lot is for commercial use.  That could be the telltale sign that even though a lot is small, it gets the \"High\" classification for likely being commercially zoned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Model\n",
    "\n",
    "The next model for analysis of the Assessed Land Value Range will be Naive Bayes (NB).  We will use the same split as before (Stratified Shuffle Split) with 5 splits utilizing 50% of the labels from each category.  As with Random Forest, we are interested in the accuracy of the model and the precision for each classification.  We feel these are important metrics because a high accuracy will tell us how viable our model is overall.  By examining precision, we can identify where our model falls short.\n",
    "\n",
    "The documentation for sklearn's Naive Bayes model recommends trying both multinomial and Bernoulli if time permits.  Time permits, so we will show both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits = 5, test_size = 0.5, train_size=0.5)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "alr = le.fit_transform(nyc_2013.assess_range)\n",
    "\n",
    "for trainidx, testidx in cv.split(X, nzd):\n",
    "    # note that these are sparse matrices\n",
    "    X_train = X.iloc[trainidx] \n",
    "    X_test = X.iloc[testidx] \n",
    "    y_train = alr[trainidx]\n",
    "    y_test = alr[testidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha is the smoothing parameter.  It can be varied from 0 to 1.\n",
    "print(\"Multinomial NB\")\n",
    "for alpha in np.arange(0.0, 1.1, 0.1):\n",
    "    clf_mnb = MultinomialNB(alpha = alpha)\n",
    "    clf_mnb.fit(X_train, y_train)\n",
    "    y_hat = clf_mnb.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_hat) * 100\n",
    "    print(\"Alpha: \", round(alpha, 1), \"\\tAccuracy:\" , round(accuracy, 6))\n",
    "\n",
    "print(\"\\nBernoulli NB\")\n",
    "for alpha in np.arange(0.0, 1.1, 0.1):\n",
    "    clf_bnb = BernoulliNB(alpha = alpha)\n",
    "    clf_bnb.fit(X_train, y_train)\n",
    "    y_hat = clf_bnb.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_hat) * 100\n",
    "    print(\"Alpha: \", round(alpha, 1), \"\\tAccuracy:\" , round(accuracy, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immediately we can see that the Bernoulli NB model outsines the Multinomial model.  While the accuracies do not change much within the two models, we do see that the best accuracy comes from an $\\alpha$ value of 0.1.  This tells us that a very small amount of Lidstone smoothing allows for a more accurate model.  The Bernoulli model is a better choice since most of our features are binary after one-hot encoding.  Bernouilli models excel when that is the case.  Going forward, we will use the Bernoulli NB model with $\\alpha$ = 0.1 as we adjust one more paremeter, fit_prior, to try to obtain even better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default value for fit_prior is true.  We'll try it with it set to false.\n",
    "clf_bnb_fpf = BernoulliNB(alpha = .1, fit_prior = False)\n",
    "clf_bnb_fpf.fit(X_train, y_train)\n",
    "y_hat = clf_bnb_fpf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_hat) * 100\n",
    "print(\"fit_prior = Flase \\tAccuracy: \", round(accuracy, 6))\n",
    "\n",
    "clf_bnb = BernoulliNB(alpha = .1, fit_prior = True)\n",
    "clf_bnb.fit(X_train, y_train)\n",
    "y_hat = clf_bnb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_hat) * 100\n",
    "print(\"fit_prior = True \\tAccuracy: \", round(accuracy, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference isn't huge, but it does seem that allowing the model to learn prior class probabilities slightly increases the accuracy.  Let's see which features carry the most weight for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights for all features\n",
    "impt_features = clf_bnb.coef_\n",
    "\n",
    "# Create a placeholder for just the top n most important\n",
    "n_features = 20\n",
    "top_impt = np.zeros( (impt_features.shape[0], n_features), dtype = 'float')\n",
    "indices = np.zeros( (impt_features.shape[0], n_features), dtype = 'int')\n",
    "\n",
    "# Define the figure\n",
    "fig, ax = plt.subplots(1, 3, figsize=(WIDTH, HEIGHT))\n",
    "\n",
    "# Extract the relevant data\n",
    "for i in range(0, impt_features.shape[0]):\n",
    "    indices[i,:] = impt_features[i, :].argsort()[:n_features]\n",
    "    top_impt[i, :] = impt_features[i, indices[i, :]]\n",
    "    #plt.xticks(range(0,10), list( X_train.columns[i] for i in impt_features[i, :].argsort()[:10]), rotation = 90)\n",
    "\n",
    "# Plot it on a subplot\n",
    "ax[0].set_title('High Features', fontdict={'size': 20})\n",
    "ax[0].bar(range(0, len(top_impt[0, :])), top_impt[0, :])\n",
    "ax[0].xaxis.set_ticks(np.arange(0, n_features))\n",
    "ax[0].set_xticklabels( list( X_train.columns[i] for i in indices[0, :]), rotation = 90)\n",
    "ax[1].set_title('Low Features', fontdict={'size': 20})\n",
    "ax[1].bar(range(len(top_impt[1, :])), top_impt[1, :])\n",
    "ax[1].xaxis.set_ticks(np.arange(0, n_features))\n",
    "ax[1].set_xticklabels( list( X_train.columns[i] for i in indices[1, :]), rotation = 90)\n",
    "#fig.subplots_adjust(hspace = .6)\n",
    "ax[2].set_title('Medium Features', fontdict={'size': 20})\n",
    "ax[2].bar(range(len(top_impt[2, :])), top_impt[2, :])\n",
    "ax[2].xaxis.set_ticks(np.arange(0, n_features))\n",
    "ax[2].set_xticklabels( list( X_train.columns[i] for i in indices[2, :]), rotation = 90)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only classification with any discernible difference in the top 20 features is for the \"High\" land value range.  We see for that category, Building Class U sits at rank 1, although the next 14 features appear to have nearly the same (if not the same) value.  Building Class U is for Utility Bureau Properties.  It surprises me why this building class carries so much weight in the decision process for High valued lots and definitely raises a few questions.  We also see Land Use Category 1 (for One & Two Family Buildings) present at rank 3.  This one makes sense to me though since \"Low\" Assessed Value constitutes the majority of data for Residential Zoning Districts.  Overwhelmingly present in the most important features are the various Fire Company codes.  We are puzzled why this feature is so significant in classifying the Assessed Land Value Range and would need to conduct addtional exploratory data analyses, possibly with the appropriate shapefiles for Fire Company, to understand how this feature became to be so important in the Naive Bayes model.\n",
    "\n",
    "Below we examine the actual values of the weights for the top 20 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_weights = pd.DataFrame(top_impt, \n",
    "                          index = [\"High\", \"Low\", \"Medium\"],\n",
    "                          columns = [\"Rank 1\", \"Rank 2\", \"Rank 3\", \"Rank 4\", \"Rank 5\", \n",
    "                                     \"Rank 6\", \"Rank 7\", \"Rank 8\", \"Rank 9\", \"Rank 10\",\n",
    "                                     \"Rank 11\", \"Rank 12\", \"Rank 13\", \"Rank 14\", \"Rank 15\",\n",
    "                                     \"Rank 16\", \"Rank 17\", \"Rank 18\", \"Rank 19\", \"Rank 20\"])\n",
    "NB_weights.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see is that most of the weights share the same value.  It isn't until we look well beyond the first 20 that we start to see any noticable difference.  Below is the same plot from above, but looking at all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the weights for all features\n",
    "impt_features = clf_bnb.coef_\n",
    "\n",
    "# Create a placeholder for just the top n most important\n",
    "n_features = impt_features.shape[1]\n",
    "top_impt = np.zeros( (impt_features.shape[0], n_features), dtype = 'float')\n",
    "indices = np.zeros( (impt_features.shape[0], n_features), dtype = 'int')\n",
    "\n",
    "# Define the figure\n",
    "fig, ax = plt.subplots(1, 3, figsize=(WIDTH, HEIGHT))\n",
    "\n",
    "# Extract the relevant data\n",
    "for i in range(0, impt_features.shape[0]):\n",
    "    indices[i,:] = impt_features[i, :].argsort()[:n_features]\n",
    "    top_impt[i, :] = impt_features[i, indices[i, :]]\n",
    "\n",
    "# Plot it on a subplot\n",
    "ax[0].set_title('High Features', fontdict={'size': 20})\n",
    "ax[0].bar(range(0, len(top_impt[0, :])), top_impt[0, :])\n",
    "ax[1].set_title('Low Features', fontdict={'size': 20})\n",
    "ax[1].bar(range(len(top_impt[1, :])), top_impt[1, :])\n",
    "ax[2].set_title('Medium Features', fontdict={'size': 20})\n",
    "ax[2].bar(range(len(top_impt[2, :])), top_impt[2, :])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gaps near 100, 300, and 500 appear to be artifacts of the plotting method and do not reflect 0 values for features in that region.  The big takeaway here though is that the NB model assigns the same weights to tens or even hundreds of features when making a classification.  It isn't until near the 400th most important feature that weights start to vary from feature to feature going forward.\n",
    "\n",
    "While this model isn't very accurate, it does compute relatively quickly.  The Bernoulli NB was a little slower than the Multinomial, but the difference still warrants the use of Bernoulli.  Since much of our data was binarized due to one-hot encoding, Bernoulli is a much better choice than Multinomial anyway.  With some further study, we may be able to do more with this model knowing that so many of the features carry the same weight.  For now though, we'll look at another model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBooster Model\n",
    "\n",
    "Our XGBooster model to predict Assessed Land Value Range will use the same split as we leveraged earlier via the Stratified Shuffle Split function from sklearn. We continue to use this split for consistency between models and to evaluate how each model performs under the same data split.  Again, the parameters we used are: 5 split iterations utilizing 50% of the labels from each category.\n",
    "\n",
    "As shown below, the model performed at an accuracy of 0.99965, and the execution TAT, including the confussion matricess remained at 25.3 seconds.\n",
    "\n",
    "We kept the max_depth parameter at 5, one level under the default which is 6 since max_depth is to control the maximum depth of a tree to avoid overfitting. We tested earlier with other values and found that execution time goes up without any gain in the accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "cv = StratifiedShuffleSplit(n_splits = 5, test_size = 0.5, train_size=0.5)\n",
    "\n",
    "# Encode the labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "alr = le.fit_transform(nyc_2013.assess_range)\n",
    "\n",
    "for trainidx, testidx in cv.split(X, alr ):\n",
    "    X_train  = X.iloc[trainidx] \n",
    "    X_test   = X.iloc[testidx] \n",
    "    y_train  =  alr[trainidx]\n",
    "    y_test   =  alr[testidx]\n",
    "\n",
    "# Define the model\n",
    "alr_xgbooster = xgb.XGBClassifier(max_depth=5)\n",
    "alr_xgbooster.fit(X_train, y_train)\n",
    "\n",
    "# Output the accuracy\n",
    "strong_accuracy_test = alr_xgbooster.score(X_test,y_test)\n",
    "print(\"ALR XGBooster Strong Accuracy at 50/50 CV split: \", strong_accuracy_test)\n",
    "\n",
    "\n",
    "# ALR XGBooster Predictions\n",
    "XGBooster_test_predictions  = pd.DataFrame(alr_xgbooster.predict(X_test))\n",
    "XGBooster_train_predictions = pd.DataFrame(alr_xgbooster.predict(X_train))\n",
    "\n",
    "\n",
    "#Confusion Matrices\n",
    "# Define the figure\n",
    "fig, ax = plt.subplots(1, 2, figsize=(WIDTH, HEIGHT))\n",
    "cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
    "ax[0].set_title('XBG Training Data Results', fontdict={'size': 20})\n",
    "ax[1].set_title('XBG Testing Data Results', fontdict={'size': 20})\n",
    "fig.text(.47, .25, 'Assess Range Predicted label', ha='center', va='center', fontdict={'size': 15})\n",
    "fig.text(-.03, 0.5, 'Assess Range True Label', ha='center', va='center', rotation='vertical', fontdict={'size': 15})\n",
    "# Labels for \"Y\" and \"N\" don't seem to load either.  Tried many different ways.\n",
    "labels = le.classes_\n",
    "mat1 = confusion_matrix(y_train, XGBooster_train_predictions)/len(y_train)*100\n",
    "mat2 = confusion_matrix(y_test, XGBooster_test_predictions)/len(y_test)*100\n",
    "\n",
    "ax[0] = sns.heatmap(mat1, \n",
    "            square=True, \n",
    "            annot=True,\n",
    "            fmt='.2g',\n",
    "            cbar_ax = cbar_ax,\n",
    "            linewidths=.5, \n",
    "            ax= ax[0])\n",
    "ax[0].set_xticklabels((labels))\n",
    "ax[0].set_yticklabels((labels))\n",
    "ax[1] = sns.heatmap(mat2, \n",
    "            square=True, \n",
    "            annot=True, \n",
    "            fmt='.2g',\n",
    "            cbar_ax = cbar_ax,\n",
    "            linewidths=.5, \n",
    "            ax= ax[1])\n",
    "ax[1].set_xticklabels((labels))\n",
    "ax[1].set_yticklabels((labels))\n",
    "fig.tight_layout(rect=[0, 0, .9, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "* **[5 points]** _How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our model, most notabley the XGBoosted Classifier with a very high accuracy between both variables, we can help clients in real estate development and commercial/residential contracting.\n",
    "\n",
    "For a contractor interested in building in the New York City area, the zoning district feature is important in understanding the type of work that needs to be done at a specific site. Coupled with Land Assessment Range,  we can help contractors select the most profitable projects. The ability to predict the type fo zoning classification, and the range of the final assessment, contractors will be able effectively bid the correct amount on NYC jobs. We could measure successful bids over time as a way to assess the value of the model. Although there are other variables associated with wins and losses, it is a good proxy for selecting bids close to the winning price.\n",
    "\n",
    "For clients interested in real estate development, we can also use the variable importance features to help assessors pinpoint the most important features of a property before sale. The company can make checklist for those out in the field to select the best properties. We can then measure the value of our model by the final sales bin price accuracy  with new data points. Although this could be regressed continuously, the range allows for people to make real world decisions and make bids purchases on real estate effectively.\n",
    "\n",
    "In addition to the data provided by the city, we could collect data not directly related to individual properties. For example, crime statistics for residential areas might help accurately predict the Land Value.\n",
    "\n",
    "For commercial zones we could collect more data on Permit Issuance Data to better understand the way the city give zoning permits. In addition, Violation data can help factor into making close to the correct land value range. With more datapoints and features, the bins for the values could be made slightly more granular to allow for a closer price when making an estimate of the price.\n",
    "\n",
    "\n",
    "\n",
    "Source: https://opendata.cityofnewyork.us\n",
    "\n",
    "We could deploy this model in the middle of a pipeline. The accessors out in the field and contractors looking for new jobs, collect live data based on the top features that are selected in the original model through variable importance measures. we can then stage the data to be run through the model regularly. We could then find the average time to complete, or average time to sell and refresh our model based on the new data. This will allow us to see bottom of the funnel of results with a dynamic date refresh. This should increase accuracy over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('./Data/Deployment.png')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work\n",
    "\n",
    "* **[10 points]** _You have free reign to provide additional analyses.  One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "190px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "629.7794189453125px",
    "left": "1137.8492431640625px",
    "right": "137.00367736816406px",
    "top": "151.91175842285156px",
    "width": "676.9117431640625px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
